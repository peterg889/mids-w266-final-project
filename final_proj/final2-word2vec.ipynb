{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peterg/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/peterg/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from importlib import reload\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import random\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary\n",
    "from w266_common import patched_numpy_io\n",
    "\n",
    "from sentence import Sentence\n",
    "from embedding import *\n",
    "from training_set import TrainingSet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KID_INAPPROPRIATE = 0\n",
    "KID_APPROPRIATE = 1\n",
    "\n",
    "labels = [KID_INAPPROPRIATE, KID_APPROPRIATE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_files = glob.glob('/home/peterg/mids-w266-final-project/final_proj/books/*.txt')\n",
    "fourchan_files = glob.glob('/home/peterg/mids-w266-final-project/final_proj/4chan/*.txt')\n",
    "single_4chan_file = [\"/home/peterg/mids-w266-final-project/final_proj/4chan/4chan_s4s.txt\",]\n",
    "single_simple_wiki_file = [\"/home/peterg/mids-w266-final-project/final_proj/wiki/simple.txt\",]\n",
    "single_normal_wiki_file = [\"/home/peterg/mids-w266-final-project/final_proj/wiki/normal.txt\",]\n",
    "\n",
    "merged_pos_files = [\"/home/peterg/mids-w266-final-project/final_proj/merged/merged_pos.txt\",]\n",
    "merged_neg_files = [\"/home/peterg/mids-w266-final-project/final_proj/merged/merged_neg.txt\",]\n",
    "\n",
    "big_merged_pos_files = [\"/home/peterg/mids-w266-final-project/final_proj/merged/merged_pos_100k.txt\",]\n",
    "big_merged_neg_files = [\"/home/peterg/mids-w266-final-project/final_proj/merged/merged_neg_100k.txt\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 20000\n",
    "# NUM_EXAMPLES = 100000\n",
    "MAX_VOCAB = 70000\n",
    "PAD_LEN = 40\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "test_frac = 0.2\n",
    "train_frac = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment_set = TrainingSet(book_files, single_4chan_file, tokenizer, NUM_EXAMPLES)\n",
    "wiki_set = TrainingSet(single_simple_wiki_file, single_normal_wiki_file, tokenizer, NUM_EXAMPLES)\n",
    "small_merge_set = TrainingSet(merged_pos_files, merged_neg_files, tokenizer, NUM_EXAMPLES)\n",
    "merge_set = TrainingSet(big_merged_pos_files, big_merged_neg_files, tokenizer, NUM_EXAMPLES)\n",
    "\n",
    "training_sets = [comment_set, wiki_set, small_merge_set, merge_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ts in training_sets:\n",
    "    ts.generate_vocab(MAX_VOCAB)\n",
    "    ts.prep_sents(PAD_LEN)\n",
    "    ts.divide_test_train(test_frac, train_frac)\n",
    "    ts.prep_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_arr = [(\"ass and titties\", KID_INAPPROPRIATE),\n",
    "(\"fuck the police\", KID_INAPPROPRIATE),\n",
    "(\"cats are nice\", KID_APPROPRIATE),\n",
    "(\"one day I saw a horse on a hill and I liked it\", KID_APPROPRIATE),\n",
    "(\"mrs. tayler is a whore\", KID_INAPPROPRIATE),\n",
    "(\"I'd like to take you someplace nice and quiet\", KID_INAPPROPRIATE),\n",
    "(\"You gay ? Proceeds to chase him that's how bullying works\", KID_INAPPROPRIATE),\n",
    "(\"transfer responsibility dad temperature earn voter impossible radiation.\", KID_APPROPRIATE),\n",
    "(\"JADE CHYNOWETH AND JOSH KILLACKY LOVETEAM PLS\", KID_APPROPRIATE),\n",
    "(\"Lol, looks like the same generic crap you come to expect from these types of shows.  Guess it doesn't get old for some people.\", KID_INAPPROPRIATE),\n",
    "(\"Disney sold the rights to this?\", KID_APPROPRIATE),\n",
    "(\"Only step up 1 and 2 were good. All of the other ones have just been corny.\", KID_APPROPRIATE),\n",
    "(\"i rather not call it step up, its another level with different camera techniques and its so Channing Tatum style\", KID_APPROPRIATE),\n",
    "(\"I love the series its very good, im waiting for 2 one\", KID_APPROPRIATE),\n",
    "(\"GREAT\", KID_APPROPRIATE),\n",
    "(\"Job negotiate set alternative little introduction apparent crazy proper used care free.\", KID_APPROPRIATE),\n",
    "(\"Oxygen identify member dependent translate else card might handful.\", KID_APPROPRIATE),\n",
    "(\"Release accompany pole general something widely fly cup detective personnel.\", KID_APPROPRIATE),\n",
    "(\"Silly largely obstacle warrior charge flavor diabetes medal.\", KID_APPROPRIATE),\n",
    "(\"We need moose back\", KID_APPROPRIATE),\n",
    "(\"Exciting time to be alive bro watching our Prez Trump vs. the DeepState/Swamp that's the deal profs to him.\", KID_APPROPRIATE),\n",
    "(\"Idk if I want to watch it or not : ^/\", KID_APPROPRIATE),\n",
    "(\"Maximum testing blanket absolutely shock until actress sex liability.\", KID_INAPPROPRIATE),\n",
    "(\"3 was the best.\", KID_APPROPRIATE),\n",
    "(\"I knew they would have a all men gay dance group\", KID_INAPPROPRIATE),\n",
    "(\"Same old garbage; I'm sick of it !\", KID_APPROPRIATE),\n",
    "(\"GAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYYYYYYYYYYYYYYYYYYYYYYYYY\",KID_INAPPROPRIATE),\n",
    "(\"Millenials = The worst generation\",KID_APPROPRIATE),\n",
    "(\"that opening statement is the most overused sentence i've seen\",KID_APPROPRIATE),\n",
    "(\"Pan spokesman via guard campaign characteristic movement expert attend garage.\",KID_APPROPRIATE),\n",
    "(\"ayyy https://youtu.be/wZwyQZDfyHY\",KID_APPROPRIATE),\n",
    "(\"WELP....the end of the step up franchise met a very disappointing end. Jesus this looks worse than most dumpster fires Ive seen\",KID_APPROPRIATE),\n",
    "(\"blacks that look white and wiggas.  yeah, looks great. (not!)\",KID_INAPPROPRIATE),\n",
    "(\"Rich And Diverse artistry- Advocating White Genocide. How is enrichment going on in South Africa?\",KID_INAPPROPRIATE),\n",
    "(\"step up <3\",KID_APPROPRIATE),\n",
    "(\"When is episode 5 coming out\",KID_APPROPRIATE),\n",
    "(\"This series is actually pretty good...watched it and was so happy I gave it a shot; I loved it! The storyline is actually stronger than I expected, and especially where they left off this first season, I'm so excited for what's to come; I hope they keep up the good work and produce even more (authentic stuff next season.\",KID_APPROPRIATE),\n",
    "(\"https://vk.com/club162181675 вступайте\",KID_APPROPRIATE),\n",
    "(\"lunch argument across expansion free govern healthy afternoon.\",KID_APPROPRIATE),\n",
    "(\"Modern day version of Fame!\",KID_APPROPRIATE),\n",
    "(\"Neyo!\",KID_APPROPRIATE),\n",
    "(\"I wasn't expecting this from YouTube but it's really good\",KID_APPROPRIATE),\n",
    "(\"The\",KID_APPROPRIATE),\n",
    "(\"what garbage i this want to be step up is more like stomp the yard\",KID_APPROPRIATE),\n",
    "(\"if you're actually thinking of watching this show... do it. i recommend it. it's so much more than what we ever got to see from step up or any dance movies at all. so much diversity & so many topics it's talking about. i was surprised at how much i actually enjoyed the show & how bad i need a (second season. just give it a try since the first 4 episodes are for free on here, i don't think y'all are gonna regret it\",KID_APPROPRIATE),\n",
    "(\"Aren’t u guys releasing more seasons and episodes??? we still need more of ‘em\",KID_APPROPRIATE),\n",
    "(\"Have Chris brown in the step up movies it’ll be dope\",KID_APPROPRIATE),\n",
    "(\"http://hikmatblogs.blogspot.com/2018/02/10-ways-to-improve-your-finances-today.html\",KID_APPROPRIATE),\n",
    "(\"No Moose no Step Up\",KID_APPROPRIATE),\n",
    "(\"Let me guess?? Suburban girl trying to prove her gangsta and most of the black men are gay.... WHACK!!!\",KID_INAPPROPRIATE),\n",
    "(\"What, no moose cameo ?\",KID_APPROPRIATE),\n",
    "(\"trash.\",KID_APPROPRIATE),\n",
    "(\"Why does everything have to be so gay  won't be watching this sick of all our black men in tv series being gay smh\",KID_INAPPROPRIATE),\n",
    "(\"Who's that girl on the trailer cover?\",KID_APPROPRIATE),\n",
    "(\"Atlanta is a n!66er filled cesspool.. i can't figure out why parents stopped teaching their children white from wrong.. why today's youth wants grey babies.. why ruin family pictures with those half n!66e kinky haired flat nosed kids..  why these fathers aren't teaching their daughters to stay off the chimpdicks.. i mean, for real, how low does your self esteem have to be to want to share your beautiful young white body with these savage nigloyds... i'm sorry your fathers have failed you girls.. I taught mine white from wrong.. my grandkids will be bright white with blonde hair and blue eyes.. not gray with kinky rugs and flat noses.. just the idea of beautiful young white girls with sweaty, greasy n!66ers up on them just makes me wanna puke.. parents.. it's time to start teaching some basic family values again.. not the kardashian kind where they just keep getting impregnated by stray chimps.. it's just disgusting... let's clean up and keep the white blood lines free from contaminants..  you young white girls... just say no.. find a decent white boy to make babies with.. you can't tell me that a n!66er is all you can get..  well, maybe the fat ugly girls.. but even you can do better.. find a fat ugly white guy..  give him that poontang .. not these foot stomping, drum banging monkeys .. talentless rappers...  all wanna pretend they are in the music business.. rap isn't music.. any tar baby can get a drum machine and call it a beat lab.. then they steal a macbook from a white girl.. and start their career as the white girl's baby daddy.. she says he's a musician.. but he's really just another worthless n166er  he will keep her on welfare.. just a waste.. february is black history month.. the other 11 is caucasian (history months.. we will celebrate by taking our women back..\",KID_INAPPROPRIATE),\n",
    "(\"i run them streetsXD\",KID_APPROPRIATE),\n",
    "(\"my answer is....hell noXD\",KID_APPROPRIATE),\n",
    "(\"Sail star diamond rate working love pond size that monument prevention celebrate.\",KID_APPROPRIATE),\n",
    "(\"So is Kevin Bacon they're Jesus or something?\",KID_APPROPRIATE),\n",
    "(\"Can someone please give me the name of the girl on the thumbnail please ?????\",KID_APPROPRIATE),\n",
    "(\"I would rather be prison raped than watch 5seconds of this garbage\",KID_INAPPROPRIATE),\n",
    "(\"Prosecution tool before endless visit dump shake sake remarkable hurt safe public.\",KID_APPROPRIATE),\n",
    "(\"total grief slave esyvbp question fourth fun basic fly sacrifice reply.\",KID_INAPPROPRIATE),\n",
    "(\"As she named the Empress, Anna Pávlovna’s face suddenly assumed an expression of profound and sincere devotion and respect mingled with sadness, and this occurred every time she mentioned her illustrious patroness. \", KID_INAPPROPRIATE),\n",
    "(\"She added that Her Majesty had deigned to show Baron Funke beaucoup d’estime, and again her face clouded over with sadness.\", KID_INAPPROPRIATE),\n",
    "(\"The prince was silent and looked indifferent. \", KID_APPROPRIATE),\n",
    "(\"But, with the womanly and courtierlike quickness and tact habitual to her, Anna Pávlovna wished both to rebuke him (for daring to speak as he had done of a ma \", KID_INAPPROPRIATE),\n",
    "(\"recommended to the Empress) and at the same time to console him, so she said \", KID_INAPPROPRIATE),\n",
    "(\"Such lame shit\", KID_INAPPROPRIATE),\n",
    "(\"Who the fuck is IcePoseidon and why do we have to watch him be stupid\", KID_INAPPROPRIATE),\n",
    "(\"Legends say he's still about to lose his job\", KID_APPROPRIATE),\n",
    "(\"WTF\", KID_INAPPROPRIATE),\n",
    "(\"Classic Greeks\", KID_APPROPRIATE),\n",
    "(\"8:27 de guy was jerking his dick\", KID_INAPPROPRIATE),\n",
    "(\"Roses are red violets are blue I just got click baited and so did you\", KID_APPROPRIATE),\n",
    "(\"Worst ad campaign I have ever seen. And this is after the 2016 elections\", KID_APPROPRIATE),\n",
    "(\"bleeps words but shows a 45 second clip of a guy masturbating. Real high standards we have here\", KID_INAPPROPRIATE),\n",
    "(\"ok Ice posiden ACIDENTLY this many times is fake\", KID_APPROPRIATE),\n",
    "(\"It seems like Ice Poseidon can’t get his shit together and stream accordingly to his schedule since he’s got the most accidental live starts. Never liked him and seeing his fuck ups just makes himself look bad\", KID_INAPPROPRIATE),\n",
    "(\"did he lose his job\", KID_APPROPRIATE),\n",
    "(\"Hahahahaha\", KID_APPROPRIATE),\n",
    "(\"greek, no one will ever suck your dick\", KID_INAPPROPRIATE),\n",
    "(\"I didn't understand most of these\", KID_APPROPRIATE),\n",
    "(\"4:04 its not accidentally ?\", KID_APPROPRIATE),\n",
    "(\"why is there 3k dislikes\", KID_APPROPRIATE),\n",
    "(\"That isn’t a dildo, that’s a motion controller.....\", KID_INAPPROPRIATE),\n",
    "(\"A GUITAR.. she tried so hard. I probably would’ve acted the same way lol\", KID_APPROPRIATE),\n",
    "(\"Why does people watch streaming people. Fucking dumb\", KID_INAPPROPRIATE),\n",
    "(\"7:52 that was merchant saying I'm to busy running hes on youtube\", KID_APPROPRIATE),\n",
    "(\"4:20 what did he do\", KID_APPROPRIATE),\n",
    "(\"who's the last girl\", KID_APPROPRIATE),\n",
    "(\"Did he lose his job though\", KID_APPROPRIATE),\n",
    "(\"Always thumbs down to these videos. Don't know why they're recommended to me. Cancerous, toxic ass puss, all of it\", KID_INAPPROPRIATE),\n",
    "(\"That's mad, I went to school with the guy hosting the RS stream at 7:30. Sat next to him in GCSE IT\", KID_APPROPRIATE),\n",
    "(\"I knew he did a lot of RS videos but not stuff like this\", KID_APPROPRIATE),\n",
    "(\"Did he lose his job\", KID_APPROPRIATE),\n",
    "(\"What the fuck was said to the girl at the end\", KID_INAPPROPRIATE),\n",
    "(\"Where the fuck does Ninga forget\", KID_INAPPROPRIATE),\n",
    "(\"Whats that under your bed? Umm karaoke mic\", KID_APPROPRIATE),\n",
    "(\"that was my frend\", KID_APPROPRIATE),\n",
    "(\"im playing\", KID_APPROPRIATE),\n",
    "(\"What’s that under your bed.  It’s a fucking jumprope\", KID_INAPPROPRIATE),\n",
    "(\"What was that thing under they girl’s bed\", KID_APPROPRIATE),\n",
    "(\"Intro song please\", KID_APPROPRIATE),\n",
    "(\"Man at 8:00 had a terrible brain freeze\", KID_APPROPRIATE),\n",
    "(\"Wait I don't get it, why are they speaking to a chat if it's not live.\", KID_APPROPRIATE),\n",
    "(\"Did he lose his job or nah\", KID_APPROPRIATE),\n",
    "(\"At 1:30 what was under her bed\", KID_APPROPRIATE),\n",
    "(\"Lol!! He jacked off on the fucking twitch can and noticed and just fucking 😂😂😂😂😂\", KID_INAPPROPRIATE),\n",
    "(\"What happened with the guy saying he gonna lose his job? WhAt was the big deal\", KID_APPROPRIATE),\n",
    "(\"CAN SOMEONE TELL ME WHAT RUNESCAPE LEAKED LOL\", KID_APPROPRIATE),\n",
    "(\"About 70% weren't accidental live streams\", KID_APPROPRIATE),\n",
    "(\"Did he lose his job\", KID_APPROPRIATE),\n",
    "(\"Could someone please tell me the BGM at around 2:13? Please, really appreciate it\", KID_APPROPRIATE),\n",
    "(\"Super\", KID_APPROPRIATE),\n",
    "(\"Who's running this fucking channel? Get rid of these titty streamer thumbnails, dude. Real gross shit especially with this latest one\", KID_INAPPROPRIATE),\n",
    "(\"Fuck em\", KID_INAPPROPRIATE),\n",
    "(\"My God it's called a bandaid cover the camera when not in use\", KID_APPROPRIATE),\n",
    "(\"Accidental my ass\", KID_INAPPROPRIATE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import models;\n",
    "\n",
    "# ts = comment_set\n",
    "ts = wiki_set\n",
    "\n",
    "def train_and_test_bow(ts):\n",
    "    start_time = time.time()\n",
    "    x, ns, y, ext_y = ts.trains\n",
    "    \n",
    "    test_set = TrainingSet()\n",
    "\n",
    "    test_set.load_test_arr(test_arr,tokenizer)\n",
    "    test_set.set_vocab(ts.vocab)\n",
    "    test_set.prep_sents(PAD_LEN)\n",
    "    test_set.prep_sets()\n",
    "\n",
    "    # Training params\n",
    "    batch_size = 64\n",
    "    model_params = dict(V=ts.vocab, embed_dim=70, hidden_dims=[75,50,25], num_classes=len(labels),\n",
    "                        encoder_type='bow',\n",
    "                        lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "    model_fn = models.classifier_model_fn\n",
    "\n",
    "    # training\n",
    "    total_batches = 0\n",
    "    total_examples = 0\n",
    "    total_loss = 0\n",
    "    loss_ema = np.log(2)  # track exponential-moving-average of loss\n",
    "    ema_decay = np.exp(-1/10)  # decay parameter for moving average = np.exp(-1/history_length)\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        ##\n",
    "        # Construct the graph here. No session.run calls - just wiring up Tensors.\n",
    "        ##\n",
    "        # Add placeholders so we can feed in data.\n",
    "        x_ph_  = tf.placeholder(tf.int32, shape=[None, x.shape[1]])  # [batch_size, max_len]\n",
    "        ns_ph_ = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "        y_ph_  = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "\n",
    "        # Construct the graph using model_fn\n",
    "        features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "        estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                                  params=model_params)\n",
    "        loss_     = estimator_spec.loss\n",
    "        train_op_ = estimator_spec.train_op\n",
    "\n",
    "        ##\n",
    "        # Done constructing the graph, now we can make session.run calls.\n",
    "        ##\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    checkpoint_dir = \"/tmp/tf_bow_sst_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    if os.path.isdir(checkpoint_dir):\n",
    "        shutil.rmtree(checkpoint_dir)\n",
    "    # Write vocabulary to file, so TensorBoard can label embeddings.\n",
    "    # creates checkpoint_dir/projector_config.pbtxt and checkpoint_dir/metadata.tsv\n",
    "    ts.vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "    model = tf.estimator.Estimator(model_fn=models.classifier_model_fn, \n",
    "                                   params=model_params,\n",
    "                                   model_dir=checkpoint_dir)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"To view training (once it starts), run:\\n\")\n",
    "    print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "    print(\"\\nThen in your browser, open: http://localhost:6006\")\n",
    "    \n",
    "    # Training params, just used in this cell for the input_fn-s\n",
    "    train_params = dict(batch_size=128, total_epochs=20, eval_every=10)\n",
    "    assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "    # Construct and train the model, saving checkpoints to the directory above.\n",
    "    # Input function for training set batches\n",
    "    # Do 'eval_every' epochs at once, followed by evaluating on the dev set.\n",
    "    # NOTE: use patch_numpy_io.numpy_input_fn instead of tf.estimator.inputs.numpy_input_fn\n",
    "    train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                        x={\"ids\": x, \"ns\": ns}, y=y,\n",
    "                        batch_size=train_params['batch_size'], \n",
    "                        num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                     )\n",
    "    \n",
    "    # Input function for dev set batches. As above, but:\n",
    "    # - Don't randomize order\n",
    "    # - Iterate exactly once (one epoch)\n",
    "    dev_x, dev_ns, dev_y, dev_y_ext = ts.devs\n",
    "    dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                        x={\"ids\": dev_x, \"ns\": dev_ns}, y=dev_y,\n",
    "                        batch_size=128, num_epochs=1, shuffle=False\n",
    "                    )\n",
    "\n",
    "    for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "        # Train for a few epochs, then evaluate on dev\n",
    "        model.train(input_fn=train_input_fn)\n",
    "        eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")\n",
    "        \n",
    "    sd_test_x, sd_test_ns, sd_test_y, _ = ts.sds\n",
    "    test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                        x={\"ids\": sd_test_x, \"ns\": sd_test_ns}, y=sd_test_y,\n",
    "                        batch_size=128, num_epochs=1, shuffle=False\n",
    "                    )\n",
    "\n",
    "    same_domain_eval = model.evaluate(input_fn=test_input_fn, name=\"eval\")\n",
    "\n",
    "\n",
    "    print(\"Accuracy on same domain test set: {:.02%}\".format(same_domain_eval['accuracy']))\n",
    "\n",
    "    \n",
    "    test_x, test_ns, test_y, test_y_ext = test_set.ext_tests\n",
    "    test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                        x={\"ids\": test_x, \"ns\": test_ns}, y=test_y,\n",
    "                        batch_size=128, num_epochs=1, shuffle=False\n",
    "                    )\n",
    "\n",
    "    new_domain_eval = model.evaluate(input_fn=test_input_fn, name=\"eval\")\n",
    "    print(\"Accuracy on generalized test set: {:.02%}\".format(new_domain_eval['accuracy']))\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score\n",
    "    predictions = list(model.predict(test_input_fn))  # list of dicts\n",
    "    y_pred = [p['max'] for p in predictions]\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    return same_domain_eval, new_domain_eval, y_pred, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_sd_comment, bow_nd_comment, bow_comment_preds, bct = train_and_test_bow(comment_set)\n",
    "# print(\"comments done\")\n",
    "# bow_sd_wiki, bow_nd_wiki, bow_wiki_preds, bwt          = train_and_test_bow(wiki_set)\n",
    "# print(\"wiki done\")\n",
    "# bow_sd_merge, bow_nd_merge, bow_merge_preds, bmt       = train_and_test_bow(merge_set)\n",
    "# print(\"merged done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"bow_sd_comment\", bow_sd_comment)\n",
    "# print(\"bow_nd_comment\", bow_nd_comment)\n",
    "# print(bct)\n",
    "\n",
    "# print(\"\\nbow_sd_wiki\", bow_sd_wiki)\n",
    "# print(\"bow_nd_wiki\", bow_nd_wiki)\n",
    "# print(bwt)\n",
    "\n",
    "# print(\"\\nbow_sd_merge\",bow_sd_merge)\n",
    "# print(\"bow_nd_merge\", bow_nd_merge)\n",
    "# print(bmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBED_DIM = 90\n",
    "CELL_SIZE = 40\n",
    "RNN_NUM = 3\n",
    "learning_rate = 0\n",
    "dropout_keep_prob = 0\n",
    "lam = 1\n",
    "train_embedding = False\n",
    "embedding_path = \"\"\n",
    "model_dir = \"runs\"\n",
    "summaries_dir = \"summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_EVERY=100\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=10\n",
      "NUM_FILTERS=128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 128, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 10)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_test_cnn(ts, use_embedding = False):\n",
    "    start_time = time.time()\n",
    "    x, _, y, y_ext = ts.trains\n",
    "    dev_x, _, dev_y, dev_y_ext = ts.devs\n",
    "    test_set = TrainingSet()\n",
    "\n",
    "    test_set.load_test_arr(test_arr,tokenizer)\n",
    "    test_set.set_vocab(ts.vocab)\n",
    "    test_set.prep_sents(PAD_LEN)\n",
    "    test_set.prep_sets()\n",
    "    \n",
    "    test_x, test_ns, test_y, test_y_ext = test_set.ext_tests\n",
    "    sd_test_x, _, sd_test_y, sd_test_y_ext = ts.sds\n",
    "    \n",
    "    # Training\n",
    "    # ==================================================\n",
    "    last_path = \"\"\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "          log_device_placement=FLAGS.log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        if use_embedding:\n",
    "            embedding_path = \"/home/peterg/word2vec-GoogleNews-vectors/GoogleNews-vectors-negative300-SLIM.bin\"\n",
    "        else:\n",
    "            embedding_path = \"\"\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x.shape[1],\n",
    "                num_classes=len(labels),\n",
    "                vocab=ts.vocab,\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters=FLAGS.num_filters,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "                embedding_path=embedding_path)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Dev summaries\n",
    "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "            # Write vocabulary\n",
    "            ts.vocab.write_flat_file(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            def train_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                if step % 100 == 0:\n",
    "                    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            def dev_step(x_batch, y_batch, writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(x, y_ext)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(dev_x, dev_y_ext, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    last_path = path\n",
    "    def test(test_x, test_y):\n",
    "    #     checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "        checkpoint_file = path\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            session_conf = tf.ConfigProto(\n",
    "              allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "              log_device_placement=FLAGS.log_device_placement)\n",
    "            sess = tf.Session(config=session_conf)\n",
    "            with sess.as_default():\n",
    "                # Load the saved meta graph and restore variables\n",
    "                saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "                saver.restore(sess, checkpoint_file)\n",
    "\n",
    "                # Get the placeholders from the graph by name\n",
    "                input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "                # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "                dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "                # Tensors we want to evaluate\n",
    "                predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "                # Generate batches for one epoch\n",
    "                batches = batch_iter(list(test_x), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "                # Collect the predictions here\n",
    "                all_predictions = []\n",
    "\n",
    "                for x_test_batch in batches:\n",
    "                    batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "                    all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "        # Print accuracy if y_test is defined\n",
    "        if test_y is not None:\n",
    "            correct_predictions = float(sum(all_predictions == test_y))\n",
    "            print(\"Total number of test examples: {}\".format(len(test_y)))\n",
    "            print(\"Accuracy: {:g}\".format(correct_predictions/float(len(test_y))))\n",
    "            return correct_predictions/float(len(test_y)), all_predictions\n",
    "    same_domain, sd_preds = test(sd_test_x, sd_test_y)\n",
    "    new_domain, nd_preds = test(test_x, test_y)\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    return same_domain, new_domain, nd_preds, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Value passed to parameter 'input' has DataType float64 not in list of allowed values: float16, float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b9eb9bc5f516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn_sd_comment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_nd_comment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_preds_comment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_test_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"comments done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcnn_sd_wiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_nd_wiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_preds_wiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwt\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_test_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wiki done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcnn_sd_merge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_nd_merge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_preds_merge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmt\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_test_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a6a6f2373109>\u001b[0m in \u001b[0;36mtrain_and_test_cnn\u001b[0;34m(ts, use_embedding)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mnum_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_filters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0ml2_reg_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_reg_lambda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 embedding_path=embedding_path)\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Define Training procedure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mids-w266-final-project/final_proj/text_cnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sequence_length, num_classes, vocab, embedding_size, filter_sizes, num_filters, l2_reg_lambda, embedding_path, init_scale)\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"VALID\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     name=\"conv\")\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0;31m# Apply nonlinearity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;34m\"Conv2D\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m         data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m    632\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    607\u001b[0m               _SatisfiesTypeConstraint(base_type,\n\u001b[1;32m    608\u001b[0m                                        \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                                        param_name=input_name)\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0minferred_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           (param_name, dtypes.as_dtype(dtype).name,\n\u001b[0;32m---> 60\u001b[0;31m            \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'input' has DataType float64 not in list of allowed values: float16, float32"
     ]
    }
   ],
   "source": [
    "cnn_sd_comment, cnn_nd_comment, cnn_preds_comment, cct = train_and_test_cnn(comment_set, True)\n",
    "print(\"comments done\")\n",
    "cnn_sd_wiki, cnn_nd_wiki, cnn_preds_wiki, cwt          = train_and_test_cnn(wiki_set, True)\n",
    "print(\"wiki done\")\n",
    "cnn_sd_merge, cnn_nd_merge, cnn_preds_merge, cmt       = train_and_test_cnn(merge_set, True)\n",
    "print(\"merge done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sd comment Accuracy: {:g}\".format(cnn_sd_comment))\n",
    "print(\"nd comment Accuracy: {:g}\".format(cnn_nd_comment))\n",
    "print(cct)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"sd wiki Accuracy: {:g}\".format(cnn_sd_wiki))\n",
    "print(\"nd wiki Accuracy: {:g}\".format(cnn_nd_wiki))\n",
    "print(cwt)\n",
    "print(\"\\n\")\n",
    "print(\"sd merge Accuracy: {:g}\".format(cnn_sd_merge))\n",
    "print(\"nd merge Accuracy: {:g}\".format(cnn_nd_merge))\n",
    "print(cmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- https://www.tensorflow.org/tutorials/text_classification_with_tf_hub\n",
    "-- https://github.com/dennybritz/cnn-text-classification-tf\n",
    "-- With word2vec\n",
    "-- https://github.com/cahya-wirawan/cnn-text-classification-tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn_sd_merge, small_cnn_nd_merge, small_cnn_preds_merge, cmt       = train_and_test_cnn(small_merge_set, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"small sd merge Accuracy: {:g}\".format(small_cnn_sd_merge))\n",
    "print(\"small nd merge Accuracy: {:g}\".format(small_cnn_nd_merge))\n",
    "print(cmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
