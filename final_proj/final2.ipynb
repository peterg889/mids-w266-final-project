{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peterg/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/peterg/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from importlib import reload\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import random\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary\n",
    "from w266_common import patched_numpy_io\n",
    "\n",
    "from sentence import Sentence\n",
    "from embedding import *\n",
    "from training_set import TrainingSet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KID_INAPPROPRIATE = 0\n",
    "KID_APPROPRIATE = 1\n",
    "\n",
    "labels = [KID_INAPPROPRIATE, KID_APPROPRIATE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_files = glob.glob('/home/peterg/mids-w266-final-project/final_proj/books/*.txt')\n",
    "fourchan_files = glob.glob('/home/peterg/mids-w266-final-project/final_proj/4chan/*.txt')\n",
    "single_4chan_file = [\"/home/peterg/mids-w266-final-project/final_proj/4chan/4chan_s4s.txt\",]\n",
    "single_simple_wiki_file = [\"/home/peterg/mids-w266-final-project/final_proj/wiki/simple.txt\",]\n",
    "single_normal_wiki_file = [\"/home/peterg/mids-w266-final-project/final_proj/wiki/normal.txt\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 1500\n",
    "# NUM_EXAMPLES = 150000\n",
    "MAX_VOCAB = 70000\n",
    "PAD_LEN = 40\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "test_frac = 0.2\n",
    "train_frac = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_set = TrainingSet(book_files, single_4chan_file, tokenizer, NUM_EXAMPLES)\n",
    "wiki_set = TrainingSet(single_simple_wiki_file, single_normal_wiki_file, tokenizer, NUM_EXAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_set.generate_vocab(MAX_VOCAB)\n",
    "wiki_set.generate_vocab(MAX_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_set.prep_sents(PAD_LEN)\n",
    "wiki_set.prep_sents(PAD_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'song', 'of', 'hiawatha', 'henry', 'w.', 'longfellow', 'contents', 'introductory', 'note', 'introduction', 'i', '.'] ['the', 'song', 'of', 'hiawatha', 'henry', 'w.', 'longfellow', 'contents', 'introductory', 'note', 'introduction', 'i', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] [5, 237, 7, 29, 1362, 2204, 1141, 2205, 1705, 1363, 1706, 24, 8] [5, 237, 7, 29, 1362, 2204, 1141, 2205, 1705, 1363, 1706, 24, 8, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "\n",
      "\n",
      "\n",
      "['it', 'is', 'the', 'county', 'seat', 'of', 'alfalfa', 'county', '.'] ['it', 'is', 'the', 'county', 'seat', 'of', 'alfalfa', 'county', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] [20, 11, 4, 57, 244, 7, 2557, 57, 6] [20, 11, 4, 57, 244, 7, 2557, 57, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "x = comment_set.pos_sentences[0]\n",
    "print(x.tokens, x.padded_tokens, x.token_ids, x.padded_tokens_ids)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "x = wiki_set.pos_sentences[0]\n",
    "print(x.tokens, x.padded_tokens, x.token_ids, x.padded_tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frac = 0.2\n",
    "train_frac = 0.8\n",
    "\n",
    "comment_set.divide_test_train(test_frac, train_frac)\n",
    "wiki_set.divide_test_train(test_frac, train_frac)\n",
    "\n",
    "comment_set.prep_sets()\n",
    "wiki_set.prep_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,600 examples, moving-average loss 1.79\n",
      "Completed one epoch in 0:00:00\n"
     ]
    }
   ],
   "source": [
    "import models;\n",
    "\n",
    "# ts = comment_set\n",
    "ts = wiki_set\n",
    "\n",
    "x, ns, y, ext_y = ts.trains\n",
    "\n",
    "# Training params\n",
    "batch_size = 64\n",
    "model_params = dict(V=ts.vocab, embed_dim=90, hidden_dims=[50,25], num_classes=len(labels),\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "model_fn = models.classifier_model_fn\n",
    "\n",
    "# training\n",
    "total_batches = 0\n",
    "total_examples = 0\n",
    "total_loss = 0\n",
    "loss_ema = np.log(2)  # track exponential-moving-average of loss\n",
    "ema_decay = np.exp(-1/10)  # decay parameter for moving average = np.exp(-1/history_length)\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    ##\n",
    "    # Construct the graph here. No session.run calls - just wiring up Tensors.\n",
    "    ##\n",
    "    # Add placeholders so we can feed in data.\n",
    "    x_ph_  = tf.placeholder(tf.int32, shape=[None, x.shape[1]])  # [batch_size, max_len]\n",
    "    ns_ph_ = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    y_ph_  = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    \n",
    "    # Construct the graph using model_fn\n",
    "    features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "    estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                              params=model_params)\n",
    "    loss_     = estimator_spec.loss\n",
    "    train_op_ = estimator_spec.train_op\n",
    "    \n",
    "    ##\n",
    "    # Done constructing the graph, now we can make session.run calls.\n",
    "    ##\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Run a single epoch\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for (bx, bns, by) in utils.multi_batch_generator(batch_size, x, ns, y):\n",
    "        # feed NumPy arrays into the placeholder Tensors\n",
    "        feed_dict = {x_ph_: bx, ns_ph_: bns, y_ph_: by}\n",
    "        batch_loss, _ = sess.run([loss_, train_op_], feed_dict=feed_dict)\n",
    "        \n",
    "        # Compute some statistics\n",
    "        total_batches += 1\n",
    "        total_examples += len(bx)\n",
    "        total_loss += batch_loss * len(bx)  # re-scale, since batch loss is mean\n",
    "        # Compute moving average to smooth out noisy per-batch loss\n",
    "        loss_ema = ema_decay * loss_ema + (1 - ema_decay) * batch_loss\n",
    "        \n",
    "        if (total_batches % 25 == 0):\n",
    "            print(\"{:5,} examples, moving-average loss {:.2f}\".format(total_examples, \n",
    "                                                                      loss_ema))    \n",
    "    print(\"Completed one epoch in {:s}\".format(utils.pretty_timedelta(since=t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (7,969 words) written to '/tmp/tf_bow_sst_20180416-0220/metadata.tsv'\n",
      "Projector config written to /tmp/tf_bow_sst_20180416-0220/projector_config.pbtxt\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_bow_sst_20180416-0220', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f11d7201630>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/tf_bow_sst_20180416-0220' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=ts.vocab, embed_dim=70, hidden_dims=[75,50,25], num_classes=2,\n",
    "                    encoder_type='bow',\n",
    "                    lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "\n",
    "checkpoint_dir = \"/tmp/tf_bow_sst_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "# Write vocabulary to file, so TensorBoard can label embeddings.\n",
    "# creates checkpoint_dir/projector_config.pbtxt and checkpoint_dir/metadata.tsv\n",
    "ts.vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "model = tf.estimator.Estimator(model_fn=models.classifier_model_fn, \n",
    "                               params=model_params,\n",
    "                               model_dir=checkpoint_dir)\n",
    "print(\"\")\n",
    "print(\"To view training (once it starts), run:\\n\")\n",
    "print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "print(\"\\nThen in your browser, open: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training params, just used in this cell for the input_fn-s\n",
    "train_params = dict(batch_size=32, total_epochs=10, eval_every=5)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "# Input function for training set batches\n",
    "# Do 'eval_every' epochs at once, followed by evaluating on the dev set.\n",
    "# NOTE: use patch_numpy_io.numpy_input_fn instead of tf.estimator.inputs.numpy_input_fn\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": x, \"ns\": ns}, y=y,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tf_bow_sst_20180416-0220/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.822403, step = 1\n",
      "INFO:tensorflow:global_step/sec: 79.7305\n",
      "INFO:tensorflow:loss = 1.3861363, step = 101 (1.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3898\n",
      "INFO:tensorflow:loss = 1.3492652, step = 201 (1.229 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 300 into /tmp/tf_bow_sst_20180416-0220/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.1903294.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-02:20:24\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180416-0220/model.ckpt-300\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-02:20:24\n",
      "INFO:tensorflow:Saving dict for global step 300: accuracy = 0.475, cross_entropy_loss = 0.7243004, global_step = 300, loss = 1.1484821\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180416-0220/model.ckpt-300\n",
      "INFO:tensorflow:Saving checkpoints for 301 into /tmp/tf_bow_sst_20180416-0220/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.0732718, step = 301\n",
      "INFO:tensorflow:global_step/sec: 77.8986\n",
      "INFO:tensorflow:loss = 0.8007479, step = 401 (1.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5018\n",
      "INFO:tensorflow:loss = 0.9094292, step = 501 (1.242 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 600 into /tmp/tf_bow_sst_20180416-0220/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.8369418.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-02:20:31\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180416-0220/model.ckpt-600\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-02:20:31\n",
      "INFO:tensorflow:Saving dict for global step 600: accuracy = 0.4125, cross_entropy_loss = 0.969981, global_step = 600, loss = 1.2591622\n"
     ]
    }
   ],
   "source": [
    "# Input function for dev set batches. As above, but:\n",
    "# - Don't randomize order\n",
    "# - Iterate exactly once (one epoch)\n",
    "dev_x, dev_ns, dev_y, dev_y_ext = ts.devs\n",
    "dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": dev_x, \"ns\": dev_ns}, y=dev_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on dev\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_arr = [(\"ass and titties\", KID_INAPPROPRIATE),\n",
    "(\"fuck the police\", KID_INAPPROPRIATE),\n",
    "(\"cats are nice\", KID_APPROPRIATE),\n",
    "(\"one day I saw a horse on a hill and I liked it\", KID_APPROPRIATE),\n",
    "(\"mrs. tayler is a whore\", KID_INAPPROPRIATE),\n",
    "(\"I'd like to take you someplace nice and quiet\", KID_INAPPROPRIATE),\n",
    "(\"You gay ? Proceeds to chase him that's how bullying works\", KID_INAPPROPRIATE),\n",
    "(\"transfer responsibility dad temperature earn voter impossible radiation.\", KID_APPROPRIATE),\n",
    "(\"JADE CHYNOWETH AND JOSH KILLACKY LOVETEAM PLS\", KID_APPROPRIATE),\n",
    "(\"Lol, looks like the same generic crap you come to expect from these types of shows.  Guess it doesn't get old for some people.\", KID_INAPPROPRIATE),\n",
    "(\"Disney sold the rights to this?\", KID_APPROPRIATE),\n",
    "(\"Only step up 1 and 2 were good. All of the other ones have just been corny.\", KID_APPROPRIATE),\n",
    "(\"i rather not call it step up, its another level with different camera techniques and its so Channing Tatum style\", KID_APPROPRIATE),\n",
    "(\"I love the series its very good, im waiting for 2 one\", KID_APPROPRIATE),\n",
    "(\"GREAT\", KID_APPROPRIATE),\n",
    "(\"Job negotiate set alternative little introduction apparent crazy proper used care free.\", KID_APPROPRIATE),\n",
    "(\"Oxygen identify member dependent translate else card might handful.\", KID_APPROPRIATE),\n",
    "(\"Release accompany pole general something widely fly cup detective personnel.\", KID_APPROPRIATE),\n",
    "(\"Silly largely obstacle warrior charge flavor diabetes medal.\", KID_APPROPRIATE),\n",
    "(\"We need moose back\", KID_APPROPRIATE),\n",
    "(\"Exciting time to be alive bro watching our Prez Trump vs. the DeepState/Swamp that's the deal profs to him.\", KID_APPROPRIATE),\n",
    "(\"Idk if I want to watch it or not : ^/\", KID_APPROPRIATE),\n",
    "(\"Maximum testing blanket absolutely shock until actress sex liability.\", KID_INAPPROPRIATE),\n",
    "(\"3 was the best.\", KID_APPROPRIATE),\n",
    "(\"I knew they would have a all men gay dance group\", KID_INAPPROPRIATE),\n",
    "(\"Same old garbage; I'm sick of it !\", KID_APPROPRIATE),\n",
    "(\"GAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYYYYYYYYYYYYYYYYYYYYYYYYY\",KID_INAPPROPRIATE),\n",
    "(\"Millenials = The worst generation\",KID_APPROPRIATE),\n",
    "(\"that opening statement is the most overused sentence i've seen\",KID_APPROPRIATE),\n",
    "(\"Pan spokesman via guard campaign characteristic movement expert attend garage.\",KID_APPROPRIATE),\n",
    "(\"ayyy https://youtu.be/wZwyQZDfyHY\",KID_APPROPRIATE),\n",
    "(\"WELP....the end of the step up franchise met a very disappointing end. Jesus this looks worse than most dumpster fires Ive seen\",KID_APPROPRIATE),\n",
    "(\"blacks that look white and wiggas.  yeah, looks great. (not!)\",KID_INAPPROPRIATE),\n",
    "(\"Rich And Diverse artistry- Advocating White Genocide. How is enrichment going on in South Africa?\",KID_INAPPROPRIATE),\n",
    "(\"step up <3\",KID_APPROPRIATE),\n",
    "(\"When is episode 5 coming out\",KID_APPROPRIATE),\n",
    "(\"This series is actually pretty good...watched it and was so happy I gave it a shot; I loved it! The storyline is actually stronger than I expected, and especially where they left off this first season, I'm so excited for what's to come; I hope they keep up the good work and produce even more (authentic stuff next season.\",KID_APPROPRIATE),\n",
    "(\"https://vk.com/club162181675 вступайте\",KID_APPROPRIATE),\n",
    "(\"lunch argument across expansion free govern healthy afternoon.\",KID_APPROPRIATE),\n",
    "(\"Modern day version of Fame!\",KID_APPROPRIATE),\n",
    "(\"Neyo!\",KID_APPROPRIATE),\n",
    "(\"I wasn't expecting this from YouTube but it's really good\",KID_APPROPRIATE),\n",
    "(\"The\",KID_APPROPRIATE),\n",
    "(\"what garbage i this want to be step up is more like stomp the yard\",KID_APPROPRIATE),\n",
    "(\"if you're actually thinking of watching this show... do it. i recommend it. it's so much more than what we ever got to see from step up or any dance movies at all. so much diversity & so many topics it's talking about. i was surprised at how much i actually enjoyed the show & how bad i need a (second season. just give it a try since the first 4 episodes are for free on here, i don't think y'all are gonna regret it\",KID_APPROPRIATE),\n",
    "(\"Aren’t u guys releasing more seasons and episodes??? we still need more of ‘em\",KID_APPROPRIATE),\n",
    "(\"Have Chris brown in the step up movies it’ll be dope\",KID_APPROPRIATE),\n",
    "(\"http://hikmatblogs.blogspot.com/2018/02/10-ways-to-improve-your-finances-today.html\",KID_APPROPRIATE),\n",
    "(\"No Moose no Step Up\",KID_APPROPRIATE),\n",
    "(\"Let me guess?? Suburban girl trying to prove her gangsta and most of the black men are gay.... WHACK!!!\",KID_INAPPROPRIATE),\n",
    "(\"What, no moose cameo ?\",KID_APPROPRIATE),\n",
    "(\"trash.\",KID_APPROPRIATE),\n",
    "(\"Why does everything have to be so gay  won't be watching this sick of all our black men in tv series being gay smh\",KID_INAPPROPRIATE),\n",
    "(\"Who's that girl on the trailer cover?\",KID_APPROPRIATE),\n",
    "(\"Atlanta is a n!66er filled cesspool.. i can't figure out why parents stopped teaching their children white from wrong.. why today's youth wants grey babies.. why ruin family pictures with those half n!66e kinky haired flat nosed kids..  why these fathers aren't teaching their daughters to stay off the chimpdicks.. i mean, for real, how low does your self esteem have to be to want to share your beautiful young white body with these savage nigloyds... i'm sorry your fathers have failed you girls.. I taught mine white from wrong.. my grandkids will be bright white with blonde hair and blue eyes.. not gray with kinky rugs and flat noses.. just the idea of beautiful young white girls with sweaty, greasy n!66ers up on them just makes me wanna puke.. parents.. it's time to start teaching some basic family values again.. not the kardashian kind where they just keep getting impregnated by stray chimps.. it's just disgusting... let's clean up and keep the white blood lines free from contaminants..  you young white girls... just say no.. find a decent white boy to make babies with.. you can't tell me that a n!66er is all you can get..  well, maybe the fat ugly girls.. but even you can do better.. find a fat ugly white guy..  give him that poontang .. not these foot stomping, drum banging monkeys .. talentless rappers...  all wanna pretend they are in the music business.. rap isn't music.. any tar baby can get a drum machine and call it a beat lab.. then they steal a macbook from a white girl.. and start their career as the white girl's baby daddy.. she says he's a musician.. but he's really just another worthless n166er  he will keep her on welfare.. just a waste.. february is black history month.. the other 11 is caucasian (history months.. we will celebrate by taking our women back..\",KID_INAPPROPRIATE),\n",
    "(\"i run them streetsXD\",KID_APPROPRIATE),\n",
    "(\"my answer is....hell noXD\",KID_APPROPRIATE),\n",
    "(\"Sail star diamond rate working love pond size that monument prevention celebrate.\",KID_APPROPRIATE),\n",
    "(\"So is Kevin Bacon they're Jesus or something?\",KID_APPROPRIATE),\n",
    "(\"Can someone please give me the name of the girl on the thumbnail please ?????\",KID_APPROPRIATE),\n",
    "(\"I would rather be prison raped than watch 5seconds of this garbage\",KID_INAPPROPRIATE),\n",
    "(\"Prosecution tool before endless visit dump shake sake remarkable hurt safe public.\",KID_APPROPRIATE),\n",
    "(\"total grief slave esyvbp question fourth fun basic fly sacrifice reply.\",KID_INAPPROPRIATE),\n",
    "(\"As she named the Empress, Anna Pávlovna’s face suddenly assumed an expression of profound and sincere devotion and respect mingled with sadness, and this occurred every time she mentioned her illustrious patroness. \", KID_INAPPROPRIATE),\n",
    "(\"She added that Her Majesty had deigned to show Baron Funke beaucoup d’estime, and again her face clouded over with sadness.\", KID_INAPPROPRIATE),\n",
    "(\"The prince was silent and looked indifferent. \", KID_APPROPRIATE),\n",
    "(\"But, with the womanly and courtierlike quickness and tact habitual to her, Anna Pávlovna wished both to rebuke him (for daring to speak as he had done of a ma \", KID_INAPPROPRIATE),\n",
    "(\"recommended to the Empress) and at the same time to console him, so she said \", KID_INAPPROPRIATE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_set = TrainingSet()\n",
    "\n",
    "test_set.load_test_arr(test_arr,tokenizer)\n",
    "test_set.set_vocab(ts.vocab)\n",
    "test_set.prep_sents(PAD_LEN)\n",
    "test_set.prep_sets()\n",
    "\n",
    "\n",
    "sd_test_x, sd_test_ns, sd_test_y, sd_test_y_ext = ts.sds\n",
    "test_x, test_ns, test_y, test_y_ext = test_set.ext_tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-04-16-02:20:32\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180416-0220/model.ckpt-600\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-02:20:32\n",
      "INFO:tensorflow:Saving dict for global step 600: accuracy = 0.385, cross_entropy_loss = 1.0315592, global_step = 600, loss = 1.3168074\n",
      "Accuracy on same domain test set: 38.50%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.385,\n",
       " 'cross_entropy_loss': 1.0315592,\n",
       " 'global_step': 600,\n",
       " 'loss': 1.3168074}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": sd_test_x, \"ns\": sd_test_ns}, y=sd_test_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"eval\")\n",
    "\n",
    "#### END(YOUR CODE) ####\n",
    "print(\"Accuracy on same domain test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-04-16-02:20:33\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180416-0220/model.ckpt-600\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-02:20:33\n",
      "INFO:tensorflow:Saving dict for global step 600: accuracy = 0.5294118, cross_entropy_loss = 0.7024518, global_step = 600, loss = 0.9120548\n",
      "Accuracy on generalized test set: 52.94%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5294118,\n",
       " 'cross_entropy_loss': 0.7024518,\n",
       " 'global_step': 600,\n",
       " 'loss': 0.9120548}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": test_x, \"ns\": test_ns}, y=test_y,\n",
    "                    batch_size=128, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"eval\")\n",
    "\n",
    "#### END(YOUR CODE) ####\n",
    "print(\"Accuracy on generalized test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180416-0220/model.ckpt-600\n",
      "Accuracy on test set: 52.94%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = list(model.predict(test_input_fn))  # list of dicts\n",
    "y_pred = [p['max'] for p in predictions]\n",
    "acc = accuracy_score(y_pred, test_y)\n",
    "print(\"Accuracy on test set: {:.02%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBED_DIM = 90\n",
    "CELL_SIZE = 40\n",
    "RNN_NUM = 3\n",
    "learning_rate = 0\n",
    "dropout_keep_prob = 0\n",
    "lam = 1\n",
    "train_embedding = False\n",
    "embedding_path = \"\"\n",
    "model_dir = \"runs\"\n",
    "summaries_dir = \"summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=10\n",
      "NUM_FILTERS=128\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 10)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 2)\n",
      "(1920, 40)\n",
      "7969\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x, _, y, y_ext = ts.trains\n",
    "dev_x, _, dev_y, dev_y_ext = ts.trains\n",
    "\n",
    "\n",
    "print(y_ext.shape)\n",
    "print(x.shape)\n",
    "print(ts.vocab.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/peterg/mids-w266-final-project/final_proj/runs/1523845235\n",
      "\n",
      "Vocabulary (7,969 words) written to '/home/peterg/mids-w266-final-project/final_proj/runs/1523845235/vocab'\n",
      "2018-04-16T02:20:36.452073: step 1, loss 0.690816, acc 0.484375\n",
      "2018-04-16T02:20:36.740816: step 2, loss 0.70384, acc 0.515625\n",
      "2018-04-16T02:20:37.020344: step 3, loss 0.693019, acc 0.515625\n",
      "2018-04-16T02:20:37.300607: step 4, loss 0.709369, acc 0.46875\n",
      "2018-04-16T02:20:37.584350: step 5, loss 0.700969, acc 0.46875\n",
      "2018-04-16T02:20:37.868682: step 6, loss 0.675269, acc 0.625\n",
      "2018-04-16T02:20:38.149622: step 7, loss 0.718491, acc 0.515625\n",
      "2018-04-16T02:20:38.447210: step 8, loss 0.691482, acc 0.53125\n",
      "2018-04-16T02:20:38.744425: step 9, loss 0.70147, acc 0.515625\n",
      "2018-04-16T02:20:39.036584: step 10, loss 0.727673, acc 0.421875\n",
      "2018-04-16T02:20:39.329249: step 11, loss 0.691893, acc 0.484375\n",
      "2018-04-16T02:20:39.617537: step 12, loss 0.698242, acc 0.53125\n",
      "2018-04-16T02:20:39.909358: step 13, loss 0.683075, acc 0.5625\n",
      "2018-04-16T02:20:40.198740: step 14, loss 0.694097, acc 0.5625\n",
      "2018-04-16T02:20:40.488205: step 15, loss 0.709781, acc 0.5\n",
      "2018-04-16T02:20:40.775905: step 16, loss 0.709467, acc 0.53125\n",
      "2018-04-16T02:20:41.068830: step 17, loss 0.706669, acc 0.484375\n",
      "2018-04-16T02:20:41.351322: step 18, loss 0.696481, acc 0.46875\n",
      "2018-04-16T02:20:41.630382: step 19, loss 0.700712, acc 0.515625\n",
      "2018-04-16T02:20:41.984838: step 20, loss 0.711139, acc 0.4375\n",
      "2018-04-16T02:20:42.368554: step 21, loss 0.684687, acc 0.5625\n",
      "2018-04-16T02:20:42.737221: step 22, loss 0.69487, acc 0.46875\n",
      "2018-04-16T02:20:43.119395: step 23, loss 0.716767, acc 0.515625\n",
      "2018-04-16T02:20:43.398476: step 24, loss 0.689346, acc 0.5\n",
      "2018-04-16T02:20:43.676865: step 25, loss 0.677711, acc 0.625\n",
      "2018-04-16T02:20:43.955895: step 26, loss 0.697554, acc 0.484375\n",
      "2018-04-16T02:20:44.245359: step 27, loss 0.690185, acc 0.5\n",
      "2018-04-16T02:20:44.539852: step 28, loss 0.696503, acc 0.515625\n",
      "2018-04-16T02:20:44.910673: step 29, loss 0.70638, acc 0.515625\n",
      "2018-04-16T02:20:45.197564: step 30, loss 0.715033, acc 0.4375\n",
      "2018-04-16T02:20:45.484166: step 31, loss 0.679412, acc 0.53125\n",
      "2018-04-16T02:20:45.769440: step 32, loss 0.676403, acc 0.5625\n",
      "2018-04-16T02:20:46.052198: step 33, loss 0.669087, acc 0.59375\n",
      "2018-04-16T02:20:46.336861: step 34, loss 0.685547, acc 0.515625\n",
      "2018-04-16T02:20:46.620632: step 35, loss 0.666781, acc 0.625\n",
      "2018-04-16T02:20:46.898736: step 36, loss 0.68074, acc 0.53125\n",
      "2018-04-16T02:20:47.210990: step 37, loss 0.677852, acc 0.515625\n",
      "2018-04-16T02:20:47.490611: step 38, loss 0.669365, acc 0.609375\n",
      "2018-04-16T02:20:47.771278: step 39, loss 0.673504, acc 0.546875\n",
      "2018-04-16T02:20:48.067771: step 40, loss 0.71174, acc 0.453125\n",
      "2018-04-16T02:20:48.348014: step 41, loss 0.681087, acc 0.515625\n",
      "2018-04-16T02:20:48.630010: step 42, loss 0.647215, acc 0.65625\n",
      "2018-04-16T02:20:48.914509: step 43, loss 0.681787, acc 0.59375\n",
      "2018-04-16T02:20:49.213933: step 44, loss 0.656385, acc 0.625\n",
      "2018-04-16T02:20:49.495145: step 45, loss 0.662737, acc 0.578125\n",
      "2018-04-16T02:20:49.775023: step 46, loss 0.656929, acc 0.546875\n",
      "2018-04-16T02:20:50.058613: step 47, loss 0.673591, acc 0.625\n",
      "2018-04-16T02:20:50.341100: step 48, loss 0.647459, acc 0.609375\n",
      "2018-04-16T02:20:50.622090: step 49, loss 0.650446, acc 0.6875\n",
      "2018-04-16T02:20:50.908111: step 50, loss 0.698831, acc 0.484375\n",
      "2018-04-16T02:20:51.209362: step 51, loss 0.658218, acc 0.5625\n",
      "2018-04-16T02:20:51.612500: step 52, loss 0.659934, acc 0.59375\n",
      "2018-04-16T02:20:51.949932: step 53, loss 0.671052, acc 0.59375\n",
      "2018-04-16T02:20:52.231501: step 54, loss 0.702041, acc 0.484375\n",
      "2018-04-16T02:20:52.511543: step 55, loss 0.665192, acc 0.609375\n",
      "2018-04-16T02:20:52.793421: step 56, loss 0.673182, acc 0.640625\n",
      "2018-04-16T02:20:53.082107: step 57, loss 0.687466, acc 0.515625\n",
      "2018-04-16T02:20:53.366220: step 58, loss 0.652306, acc 0.65625\n",
      "2018-04-16T02:20:53.656051: step 59, loss 0.674963, acc 0.65625\n",
      "2018-04-16T02:20:53.943929: step 60, loss 0.703369, acc 0.5\n",
      "2018-04-16T02:20:54.227392: step 61, loss 0.595954, acc 0.734375\n",
      "2018-04-16T02:20:54.512832: step 62, loss 0.653164, acc 0.609375\n",
      "2018-04-16T02:20:54.796411: step 63, loss 0.657185, acc 0.609375\n",
      "2018-04-16T02:20:55.079581: step 64, loss 0.621113, acc 0.71875\n",
      "2018-04-16T02:20:55.360065: step 65, loss 0.640457, acc 0.609375\n",
      "2018-04-16T02:20:55.652237: step 66, loss 0.627495, acc 0.625\n",
      "2018-04-16T02:20:55.933709: step 67, loss 0.608902, acc 0.78125\n",
      "2018-04-16T02:20:56.214894: step 68, loss 0.644235, acc 0.671875\n",
      "2018-04-16T02:20:56.514114: step 69, loss 0.601664, acc 0.734375\n",
      "2018-04-16T02:20:56.808831: step 70, loss 0.599673, acc 0.734375\n",
      "2018-04-16T02:20:57.089574: step 71, loss 0.639103, acc 0.640625\n",
      "2018-04-16T02:20:57.370474: step 72, loss 0.603526, acc 0.671875\n",
      "2018-04-16T02:20:57.657420: step 73, loss 0.636809, acc 0.625\n",
      "2018-04-16T02:20:57.935737: step 74, loss 0.602711, acc 0.71875\n",
      "2018-04-16T02:20:58.215766: step 75, loss 0.624144, acc 0.6875\n",
      "2018-04-16T02:20:58.495548: step 76, loss 0.632199, acc 0.625\n",
      "2018-04-16T02:20:58.774514: step 77, loss 0.623024, acc 0.65625\n",
      "2018-04-16T02:20:59.055939: step 78, loss 0.57952, acc 0.671875\n",
      "2018-04-16T02:20:59.338405: step 79, loss 0.633659, acc 0.71875\n",
      "2018-04-16T02:20:59.618338: step 80, loss 0.652448, acc 0.609375\n",
      "2018-04-16T02:20:59.906728: step 81, loss 0.662658, acc 0.609375\n",
      "2018-04-16T02:21:00.192046: step 82, loss 0.664304, acc 0.609375\n",
      "2018-04-16T02:21:00.475411: step 83, loss 0.611185, acc 0.65625\n",
      "2018-04-16T02:21:00.764859: step 84, loss 0.633805, acc 0.65625\n",
      "2018-04-16T02:21:01.064904: step 85, loss 0.657314, acc 0.59375\n",
      "2018-04-16T02:21:01.347992: step 86, loss 0.630277, acc 0.640625\n",
      "2018-04-16T02:21:01.629113: step 87, loss 0.666511, acc 0.515625\n",
      "2018-04-16T02:21:01.911663: step 88, loss 0.675009, acc 0.53125\n",
      "2018-04-16T02:21:02.191531: step 89, loss 0.610053, acc 0.6875\n",
      "2018-04-16T02:21:02.474286: step 90, loss 0.605616, acc 0.625\n",
      "2018-04-16T02:21:02.764612: step 91, loss 0.582692, acc 0.8125\n",
      "2018-04-16T02:21:03.059868: step 92, loss 0.577754, acc 0.71875\n",
      "2018-04-16T02:21:03.343359: step 93, loss 0.558207, acc 0.796875\n",
      "2018-04-16T02:21:03.623357: step 94, loss 0.525988, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T02:21:03.905597: step 95, loss 0.563653, acc 0.75\n",
      "2018-04-16T02:21:04.197422: step 96, loss 0.595382, acc 0.671875\n",
      "2018-04-16T02:21:04.476307: step 97, loss 0.601663, acc 0.671875\n",
      "2018-04-16T02:21:04.765507: step 98, loss 0.57107, acc 0.796875\n",
      "2018-04-16T02:21:05.046015: step 99, loss 0.539371, acc 0.765625\n",
      "2018-04-16T02:21:05.331472: step 100, loss 0.551375, acc 0.765625\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T02:21:07.411206: step 100, loss 0.521785, acc 0.827604\n",
      "\n",
      "Saved model checkpoint to /home/peterg/mids-w266-final-project/final_proj/runs/1523845235/checkpoints/model-100\n",
      "\n",
      "2018-04-16T02:21:07.911048: step 101, loss 0.537428, acc 0.796875\n",
      "2018-04-16T02:21:08.261921: step 102, loss 0.567793, acc 0.703125\n",
      "2018-04-16T02:21:08.652906: step 103, loss 0.508657, acc 0.875\n",
      "2018-04-16T02:21:08.946237: step 104, loss 0.553952, acc 0.75\n",
      "2018-04-16T02:21:09.237971: step 105, loss 0.547054, acc 0.78125\n",
      "2018-04-16T02:21:09.530860: step 106, loss 0.565306, acc 0.6875\n",
      "2018-04-16T02:21:09.822501: step 107, loss 0.544545, acc 0.75\n",
      "2018-04-16T02:21:10.107602: step 108, loss 0.580186, acc 0.6875\n",
      "2018-04-16T02:21:10.393894: step 109, loss 0.486263, acc 0.796875\n",
      "2018-04-16T02:21:10.680064: step 110, loss 0.562077, acc 0.671875\n",
      "2018-04-16T02:21:10.964525: step 111, loss 0.611377, acc 0.640625\n",
      "2018-04-16T02:21:11.254765: step 112, loss 0.519338, acc 0.75\n",
      "2018-04-16T02:21:11.547754: step 113, loss 0.549508, acc 0.75\n",
      "2018-04-16T02:21:11.838762: step 114, loss 0.549848, acc 0.78125\n",
      "2018-04-16T02:21:12.122732: step 115, loss 0.57027, acc 0.65625\n",
      "2018-04-16T02:21:12.410590: step 116, loss 0.639619, acc 0.59375\n",
      "2018-04-16T02:21:12.697661: step 117, loss 0.606057, acc 0.71875\n",
      "2018-04-16T02:21:12.981491: step 118, loss 0.6293, acc 0.5625\n",
      "2018-04-16T02:21:13.280105: step 119, loss 0.568052, acc 0.703125\n",
      "2018-04-16T02:21:13.566233: step 120, loss 0.618812, acc 0.671875\n",
      "2018-04-16T02:21:13.856084: step 121, loss 0.469367, acc 0.8125\n",
      "2018-04-16T02:21:14.137707: step 122, loss 0.423293, acc 0.84375\n",
      "2018-04-16T02:21:14.424549: step 123, loss 0.44318, acc 0.84375\n",
      "2018-04-16T02:21:14.718674: step 124, loss 0.518076, acc 0.71875\n",
      "2018-04-16T02:21:15.000167: step 125, loss 0.508062, acc 0.75\n",
      "2018-04-16T02:21:15.285528: step 126, loss 0.498808, acc 0.75\n",
      "2018-04-16T02:21:15.566309: step 127, loss 0.406295, acc 0.875\n",
      "2018-04-16T02:21:15.850899: step 128, loss 0.450108, acc 0.875\n",
      "2018-04-16T02:21:16.132705: step 129, loss 0.445712, acc 0.796875\n",
      "2018-04-16T02:21:16.418725: step 130, loss 0.479306, acc 0.828125\n",
      "2018-04-16T02:21:16.700355: step 131, loss 0.436879, acc 0.84375\n",
      "2018-04-16T02:21:16.982008: step 132, loss 0.459068, acc 0.84375\n",
      "2018-04-16T02:21:17.269535: step 133, loss 0.475549, acc 0.765625\n",
      "2018-04-16T02:21:17.559927: step 134, loss 0.412388, acc 0.8125\n",
      "2018-04-16T02:21:17.843457: step 135, loss 0.568557, acc 0.671875\n",
      "2018-04-16T02:21:18.131508: step 136, loss 0.509583, acc 0.765625\n",
      "2018-04-16T02:21:18.414347: step 137, loss 0.477308, acc 0.8125\n",
      "2018-04-16T02:21:18.694356: step 138, loss 0.516623, acc 0.6875\n",
      "2018-04-16T02:21:18.974694: step 139, loss 0.391245, acc 0.84375\n",
      "2018-04-16T02:21:19.258109: step 140, loss 0.533812, acc 0.6875\n",
      "2018-04-16T02:21:19.539714: step 141, loss 0.436716, acc 0.828125\n",
      "2018-04-16T02:21:19.824623: step 142, loss 0.510318, acc 0.75\n",
      "2018-04-16T02:21:20.108012: step 143, loss 0.506257, acc 0.75\n",
      "2018-04-16T02:21:20.404763: step 144, loss 0.547839, acc 0.703125\n",
      "2018-04-16T02:21:20.686527: step 145, loss 0.4452, acc 0.828125\n",
      "2018-04-16T02:21:20.973694: step 146, loss 0.469798, acc 0.75\n",
      "2018-04-16T02:21:21.256019: step 147, loss 0.438442, acc 0.765625\n",
      "2018-04-16T02:21:21.537547: step 148, loss 0.516958, acc 0.71875\n",
      "2018-04-16T02:21:21.825739: step 149, loss 0.532433, acc 0.734375\n",
      "2018-04-16T02:21:22.109610: step 150, loss 0.427198, acc 0.8125\n",
      "2018-04-16T02:21:22.394517: step 151, loss 0.361346, acc 0.796875\n",
      "2018-04-16T02:21:22.673757: step 152, loss 0.339628, acc 0.875\n",
      "2018-04-16T02:21:22.954848: step 153, loss 0.281901, acc 0.921875\n",
      "2018-04-16T02:21:23.241022: step 154, loss 0.312752, acc 0.921875\n",
      "2018-04-16T02:21:23.523449: step 155, loss 0.349005, acc 0.859375\n",
      "2018-04-16T02:21:23.807304: step 156, loss 0.427432, acc 0.796875\n",
      "2018-04-16T02:21:24.089657: step 157, loss 0.448175, acc 0.75\n",
      "2018-04-16T02:21:24.374045: step 158, loss 0.438406, acc 0.796875\n",
      "2018-04-16T02:21:24.653802: step 159, loss 0.348824, acc 0.859375\n",
      "2018-04-16T02:21:24.932053: step 160, loss 0.337192, acc 0.890625\n",
      "2018-04-16T02:21:25.212765: step 161, loss 0.352248, acc 0.8125\n",
      "2018-04-16T02:21:25.493815: step 162, loss 0.404675, acc 0.828125\n",
      "2018-04-16T02:21:25.774016: step 163, loss 0.413385, acc 0.8125\n",
      "2018-04-16T02:21:26.056139: step 164, loss 0.425022, acc 0.828125\n",
      "2018-04-16T02:21:26.341782: step 165, loss 0.403376, acc 0.796875\n",
      "2018-04-16T02:21:26.623386: step 166, loss 0.393618, acc 0.828125\n",
      "2018-04-16T02:21:26.908922: step 167, loss 0.39536, acc 0.78125\n",
      "2018-04-16T02:21:27.190333: step 168, loss 0.358532, acc 0.859375\n",
      "2018-04-16T02:21:27.471135: step 169, loss 0.354761, acc 0.84375\n",
      "2018-04-16T02:21:27.752355: step 170, loss 0.39597, acc 0.859375\n",
      "2018-04-16T02:21:28.034554: step 171, loss 0.340127, acc 0.84375\n",
      "2018-04-16T02:21:28.319529: step 172, loss 0.448482, acc 0.765625\n",
      "2018-04-16T02:21:28.599297: step 173, loss 0.403172, acc 0.78125\n",
      "2018-04-16T02:21:28.879508: step 174, loss 0.361372, acc 0.828125\n",
      "2018-04-16T02:21:29.227003: step 175, loss 0.499491, acc 0.703125\n",
      "2018-04-16T02:21:29.658651: step 176, loss 0.384522, acc 0.796875\n",
      "2018-04-16T02:21:29.936733: step 177, loss 0.323497, acc 0.875\n",
      "2018-04-16T02:21:30.217707: step 178, loss 0.440875, acc 0.78125\n",
      "2018-04-16T02:21:30.496163: step 179, loss 0.425269, acc 0.828125\n",
      "2018-04-16T02:21:30.778917: step 180, loss 0.488498, acc 0.78125\n",
      "2018-04-16T02:21:31.062739: step 181, loss 0.315842, acc 0.859375\n",
      "2018-04-16T02:21:31.341845: step 182, loss 0.356236, acc 0.84375\n",
      "2018-04-16T02:21:31.651515: step 183, loss 0.258322, acc 0.859375\n",
      "2018-04-16T02:21:32.023763: step 184, loss 0.274455, acc 0.921875\n",
      "2018-04-16T02:21:32.410171: step 185, loss 0.33426, acc 0.828125\n",
      "2018-04-16T02:21:32.793751: step 186, loss 0.251877, acc 0.859375\n",
      "2018-04-16T02:21:33.186411: step 187, loss 0.408529, acc 0.75\n",
      "2018-04-16T02:21:33.571573: step 188, loss 0.36317, acc 0.8125\n",
      "2018-04-16T02:21:33.966842: step 189, loss 0.298676, acc 0.859375\n",
      "2018-04-16T02:21:34.357275: step 190, loss 0.214032, acc 0.9375\n",
      "2018-04-16T02:21:34.654752: step 191, loss 0.263714, acc 0.90625\n",
      "2018-04-16T02:21:34.938126: step 192, loss 0.218463, acc 0.9375\n",
      "2018-04-16T02:21:35.219472: step 193, loss 0.214314, acc 0.9375\n",
      "2018-04-16T02:21:35.502813: step 194, loss 0.375586, acc 0.796875\n",
      "2018-04-16T02:21:35.785291: step 195, loss 0.353368, acc 0.828125\n",
      "2018-04-16T02:21:36.068134: step 196, loss 0.260921, acc 0.90625\n",
      "2018-04-16T02:21:36.354060: step 197, loss 0.309055, acc 0.828125\n",
      "2018-04-16T02:21:36.641498: step 198, loss 0.280445, acc 0.875\n",
      "2018-04-16T02:21:36.923927: step 199, loss 0.331192, acc 0.84375\n",
      "2018-04-16T02:21:37.211464: step 200, loss 0.37013, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T02:21:39.169201: step 200, loss 0.243866, acc 0.894271\n",
      "\n",
      "Saved model checkpoint to /home/peterg/mids-w266-final-project/final_proj/runs/1523845235/checkpoints/model-200\n",
      "\n",
      "2018-04-16T02:21:39.529715: step 201, loss 0.316214, acc 0.84375\n",
      "2018-04-16T02:21:39.816207: step 202, loss 0.354192, acc 0.8125\n",
      "2018-04-16T02:21:40.100284: step 203, loss 0.416484, acc 0.796875\n",
      "2018-04-16T02:21:40.388190: step 204, loss 0.373441, acc 0.78125\n",
      "2018-04-16T02:21:40.668246: step 205, loss 0.358044, acc 0.84375\n",
      "2018-04-16T02:21:40.949164: step 206, loss 0.298905, acc 0.8125\n",
      "2018-04-16T02:21:41.232513: step 207, loss 0.287856, acc 0.859375\n",
      "2018-04-16T02:21:41.520761: step 208, loss 0.442746, acc 0.765625\n",
      "2018-04-16T02:21:41.803607: step 209, loss 0.383674, acc 0.828125\n",
      "2018-04-16T02:21:42.085753: step 210, loss 0.273745, acc 0.90625\n",
      "2018-04-16T02:21:42.369536: step 211, loss 0.260489, acc 0.875\n",
      "2018-04-16T02:21:42.655106: step 212, loss 0.16461, acc 1\n",
      "2018-04-16T02:21:42.944114: step 213, loss 0.220805, acc 0.921875\n",
      "2018-04-16T02:21:43.229189: step 214, loss 0.211599, acc 0.921875\n",
      "2018-04-16T02:21:43.510682: step 215, loss 0.318192, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T02:21:43.790956: step 216, loss 0.284216, acc 0.84375\n",
      "2018-04-16T02:21:44.073293: step 217, loss 0.228443, acc 0.890625\n",
      "2018-04-16T02:21:44.355817: step 218, loss 0.233113, acc 0.90625\n",
      "2018-04-16T02:21:44.636650: step 219, loss 0.221548, acc 0.859375\n",
      "2018-04-16T02:21:44.915887: step 220, loss 0.291929, acc 0.859375\n",
      "2018-04-16T02:21:45.199265: step 221, loss 0.250575, acc 0.84375\n",
      "2018-04-16T02:21:45.481218: step 222, loss 0.244447, acc 0.921875\n",
      "2018-04-16T02:21:45.766302: step 223, loss 0.332786, acc 0.8125\n",
      "2018-04-16T02:21:46.047944: step 224, loss 0.240679, acc 0.875\n",
      "2018-04-16T02:21:46.327267: step 225, loss 0.286028, acc 0.84375\n",
      "2018-04-16T02:21:46.627604: step 226, loss 0.288934, acc 0.8125\n",
      "2018-04-16T02:21:47.006869: step 227, loss 0.249216, acc 0.875\n",
      "2018-04-16T02:21:47.295828: step 228, loss 0.314726, acc 0.828125\n",
      "2018-04-16T02:21:47.576116: step 229, loss 0.265601, acc 0.859375\n",
      "2018-04-16T02:21:47.858757: step 230, loss 0.329729, acc 0.828125\n",
      "2018-04-16T02:21:48.141127: step 231, loss 0.258857, acc 0.890625\n",
      "2018-04-16T02:21:48.426578: step 232, loss 0.300125, acc 0.828125\n",
      "2018-04-16T02:21:48.707479: step 233, loss 0.335072, acc 0.8125\n",
      "2018-04-16T02:21:48.989100: step 234, loss 0.418482, acc 0.765625\n",
      "2018-04-16T02:21:49.292664: step 235, loss 0.339939, acc 0.8125\n",
      "2018-04-16T02:21:49.573719: step 236, loss 0.278466, acc 0.859375\n",
      "2018-04-16T02:21:49.854666: step 237, loss 0.196855, acc 0.875\n",
      "2018-04-16T02:21:50.136005: step 238, loss 0.33643, acc 0.84375\n",
      "2018-04-16T02:21:50.416261: step 239, loss 0.439415, acc 0.78125\n",
      "2018-04-16T02:21:50.698416: step 240, loss 0.426599, acc 0.75\n",
      "2018-04-16T02:21:50.986783: step 241, loss 0.124633, acc 0.984375\n",
      "2018-04-16T02:21:51.270192: step 242, loss 0.220175, acc 0.890625\n",
      "2018-04-16T02:21:51.550339: step 243, loss 0.204154, acc 0.90625\n",
      "2018-04-16T02:21:51.829104: step 244, loss 0.255938, acc 0.859375\n",
      "2018-04-16T02:21:52.113451: step 245, loss 0.262822, acc 0.859375\n",
      "2018-04-16T02:21:52.395743: step 246, loss 0.14377, acc 0.953125\n",
      "2018-04-16T02:21:52.675398: step 247, loss 0.219699, acc 0.875\n",
      "2018-04-16T02:21:52.953265: step 248, loss 0.220176, acc 0.90625\n",
      "2018-04-16T02:21:53.234166: step 249, loss 0.242245, acc 0.84375\n",
      "2018-04-16T02:21:53.512693: step 250, loss 0.158799, acc 0.921875\n",
      "2018-04-16T02:21:53.792802: step 251, loss 0.234253, acc 0.875\n",
      "2018-04-16T02:21:54.073036: step 252, loss 0.277497, acc 0.84375\n",
      "2018-04-16T02:21:54.352520: step 253, loss 0.274092, acc 0.84375\n",
      "2018-04-16T02:21:54.631967: step 254, loss 0.215216, acc 0.859375\n",
      "2018-04-16T02:21:54.913556: step 255, loss 0.185171, acc 0.90625\n",
      "2018-04-16T02:21:55.195525: step 256, loss 0.309336, acc 0.84375\n",
      "2018-04-16T02:21:55.473882: step 257, loss 0.218605, acc 0.890625\n",
      "2018-04-16T02:21:55.752962: step 258, loss 0.319467, acc 0.828125\n",
      "2018-04-16T02:21:56.037713: step 259, loss 0.142781, acc 0.953125\n",
      "2018-04-16T02:21:56.320086: step 260, loss 0.349136, acc 0.84375\n",
      "2018-04-16T02:21:56.598270: step 261, loss 0.179432, acc 0.921875\n",
      "2018-04-16T02:21:56.877163: step 262, loss 0.306155, acc 0.84375\n",
      "2018-04-16T02:21:57.155797: step 263, loss 0.324654, acc 0.828125\n",
      "2018-04-16T02:21:57.442382: step 264, loss 0.329322, acc 0.84375\n",
      "2018-04-16T02:21:57.721457: step 265, loss 0.279849, acc 0.828125\n",
      "2018-04-16T02:21:58.000574: step 266, loss 0.313147, acc 0.8125\n",
      "2018-04-16T02:21:58.282448: step 267, loss 0.186629, acc 0.921875\n",
      "2018-04-16T02:21:58.562441: step 268, loss 0.184341, acc 0.921875\n",
      "2018-04-16T02:21:58.840628: step 269, loss 0.281258, acc 0.859375\n",
      "2018-04-16T02:21:59.118802: step 270, loss 0.353188, acc 0.828125\n",
      "2018-04-16T02:21:59.402744: step 271, loss 0.191882, acc 0.921875\n",
      "2018-04-16T02:21:59.679703: step 272, loss 0.125923, acc 0.984375\n",
      "2018-04-16T02:21:59.967897: step 273, loss 0.176584, acc 0.921875\n",
      "2018-04-16T02:22:00.249611: step 274, loss 0.214966, acc 0.90625\n",
      "2018-04-16T02:22:00.528302: step 275, loss 0.192886, acc 0.9375\n",
      "2018-04-16T02:22:00.807812: step 276, loss 0.252728, acc 0.859375\n",
      "2018-04-16T02:22:01.086461: step 277, loss 0.249614, acc 0.875\n",
      "2018-04-16T02:22:01.366674: step 278, loss 0.186007, acc 0.90625\n",
      "2018-04-16T02:22:01.645895: step 279, loss 0.229487, acc 0.90625\n",
      "2018-04-16T02:22:01.925790: step 280, loss 0.211356, acc 0.859375\n",
      "2018-04-16T02:22:02.209204: step 281, loss 0.197996, acc 0.90625\n",
      "2018-04-16T02:22:02.490085: step 282, loss 0.224247, acc 0.859375\n",
      "2018-04-16T02:22:02.771918: step 283, loss 0.271445, acc 0.84375\n",
      "2018-04-16T02:22:03.053044: step 284, loss 0.163092, acc 0.890625\n",
      "2018-04-16T02:22:03.336799: step 285, loss 0.208217, acc 0.875\n",
      "2018-04-16T02:22:03.616876: step 286, loss 0.248568, acc 0.828125\n",
      "2018-04-16T02:22:03.895997: step 287, loss 0.208541, acc 0.921875\n",
      "2018-04-16T02:22:04.182302: step 288, loss 0.339793, acc 0.796875\n",
      "2018-04-16T02:22:04.459703: step 289, loss 0.263179, acc 0.859375\n",
      "2018-04-16T02:22:04.737777: step 290, loss 0.180642, acc 0.890625\n",
      "2018-04-16T02:22:05.024348: step 291, loss 0.309408, acc 0.828125\n",
      "2018-04-16T02:22:05.306574: step 292, loss 0.197106, acc 0.859375\n",
      "2018-04-16T02:22:05.586477: step 293, loss 0.248508, acc 0.828125\n",
      "2018-04-16T02:22:05.877159: step 294, loss 0.261083, acc 0.828125\n",
      "2018-04-16T02:22:06.158000: step 295, loss 0.436338, acc 0.71875\n",
      "2018-04-16T02:22:06.439133: step 296, loss 0.27879, acc 0.828125\n",
      "2018-04-16T02:22:06.721110: step 297, loss 0.291032, acc 0.828125\n",
      "2018-04-16T02:22:06.999283: step 298, loss 0.357649, acc 0.78125\n",
      "2018-04-16T02:22:07.278953: step 299, loss 0.115013, acc 0.953125\n",
      "2018-04-16T02:22:07.558393: step 300, loss 0.288049, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T02:22:09.701247: step 300, loss 0.166949, acc 0.915104\n",
      "\n",
      "Saved model checkpoint to /home/peterg/mids-w266-final-project/final_proj/runs/1523845235/checkpoints/model-300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "last_path = \"\"\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x.shape[1],\n",
    "#             num_classes=y.shape[1],\n",
    "            num_classes=len(labels),\n",
    "            vocab=ts.vocab,\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        ts.vocab.write_flat_file(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x, y_ext)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(dev_x, dev_y_ext, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                last_path = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, _, test_y, test_y_ext = test_set.ext_tests\n",
    "sd_test_x, _, sd_test_y, sd_test_y_ext = ts.sds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_x, test_y):\n",
    "#     checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "    checkpoint_file = path\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "          log_device_placement=FLAGS.log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            # Load the saved meta graph and restore variables\n",
    "            saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "            saver.restore(sess, checkpoint_file)\n",
    "\n",
    "            # Get the placeholders from the graph by name\n",
    "            input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "            # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "            dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "            # Tensors we want to evaluate\n",
    "            predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "            # Generate batches for one epoch\n",
    "            batches = batch_iter(list(test_x), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "            # Collect the predictions here\n",
    "            all_predictions = []\n",
    "\n",
    "            for x_test_batch in batches:\n",
    "                batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "                all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "    # Print accuracy if y_test is defined\n",
    "    if test_y is not None:\n",
    "        correct_predictions = float(sum(all_predictions == test_y))\n",
    "        print(\"Total number of test examples: {}\".format(len(test_y)))\n",
    "        print(\"Accuracy: {:g}\".format(correct_predictions/float(len(test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/peterg/mids-w266-final-project/final_proj/runs/1523845235/checkpoints/model-300\n",
      "Total number of test examples: 600\n",
      "Accuracy: 0.34\n"
     ]
    }
   ],
   "source": [
    "test(sd_test_x, sd_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/peterg/mids-w266-final-project/final_proj/runs/1523845235/checkpoints/model-300\n",
      "Total number of test examples: 68\n",
      "Accuracy: 0.485294\n"
     ]
    }
   ],
   "source": [
    "test(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- https://www.tensorflow.org/tutorials/text_classification_with_tf_hub\n",
    "-- https://github.com/dennybritz/cnn-text-classification-tf\n",
    "-- With word2vec\n",
    "-- https://github.com/cahya-wirawan/cnn-text-classification-tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
