{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peterg/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/peterg/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from importlib import reload\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import random\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary\n",
    "from w266_common import patched_numpy_io\n",
    "\n",
    "from sentence import Sentence\n",
    "from embedding import *\n",
    "from training_set import TrainingSet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KID_INAPPROPRIATE = 0\n",
    "KID_APPROPRIATE = 1\n",
    "\n",
    "labels = [KID_INAPPROPRIATE, KID_APPROPRIATE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_files = glob.glob('/home/peterg/mids-w266-final-project/final_proj/books/*.txt')\n",
    "fourchan_files = glob.glob('/home/peterg/mids-w266-final-project/final_proj/4chan/*.txt')\n",
    "single_4chan_file = [\"/home/peterg/mids-w266-final-project/final_proj/4chan/4chan_s4s.txt\",]\n",
    "single_simple_wiki_file = [\"/home/peterg/mids-w266-final-project/final_proj/wiki/simple.txt\",]\n",
    "single_normal_wiki_file = [\"/home/peterg/mids-w266-final-project/final_proj/wiki/normal.txt\",]\n",
    "\n",
    "merged_pos_files = [\"/home/peterg/mids-w266-final-project/final_proj/merged/merged_pos.txt\",]\n",
    "merged_neg_files = [\"/home/peterg/mids-w266-final-project/final_proj/merged/merged_neg.txt\",]\n",
    "\n",
    "big_merged_pos_files = [\"/home/peterg/mids-w266-final-project/final_proj/merged/merged_pos_100k.txt\",]\n",
    "big_merged_neg_files = [\"/home/peterg/mids-w266-final-project/final_proj/merged/merged_neg_100k.txt\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 20000\n",
    "# NUM_EXAMPLES = 150000\n",
    "MAX_VOCAB = 70000\n",
    "PAD_LEN = 40\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "test_frac = 0.2\n",
    "train_frac = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_set = TrainingSet(book_files, single_4chan_file, tokenizer, NUM_EXAMPLES)\n",
    "wiki_set = TrainingSet(single_simple_wiki_file, single_normal_wiki_file, tokenizer, NUM_EXAMPLES)\n",
    "merge_set = TrainingSet(merged_pos_files, merged_neg_files, tokenizer, NUM_EXAMPLES)\n",
    "#merge_set = TrainingSet(big_merged_pos_files, big_merged_neg_files, tokenizer, NUM_EXAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_set.generate_vocab(MAX_VOCAB)\n",
    "wiki_set.generate_vocab(MAX_VOCAB)\n",
    "merge_set.generate_vocab(MAX_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_set.prep_sents(PAD_LEN)\n",
    "wiki_set.prep_sents(PAD_LEN)\n",
    "merge_set.prep_sents(PAD_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'song', 'of', 'hiawatha', 'henry', 'w.', 'longfellow', 'contents', 'introductory', 'note', 'introduction', 'i', '.'] ['the', 'song', 'of', 'hiawatha', 'henry', 'w.', 'longfellow', 'contents', 'introductory', 'note', 'introduction', 'i', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] [5, 583, 8, 258, 1430, 916, 2733, 5232, 3282, 1601, 1722, 15, 7] [5, 583, 8, 258, 1430, 916, 2733, 5232, 3282, 1601, 1722, 15, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "\n",
      "\n",
      "\n",
      "['it', 'is', 'the', 'county', 'seat', 'of', 'alfalfa', 'county', '.'] ['it', 'is', 'the', 'county', 'seat', 'of', 'alfalfa', 'county', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] [20, 11, 4, 82, 512, 7, 11780, 82, 6] [20, 11, 4, 82, 512, 7, 11780, 82, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "\n",
      "\n",
      "\n",
      "['produced', 'by', 'anthony', 'matonac', 'and', 'paul', 'selkirk', 'the', 'patchwork', 'girl', 'of', 'oz', 'by', 'l.', 'frank', 'baum', 'affectionately', 'dedicated', 'to', 'my', 'young', 'friend', 'sumner', 'hamilton', 'britton', 'of', 'chicago', 'prologue', 'through', 'the', 'kindness', 'of', 'dorothy', 'gale', 'of', 'kansas', ',', 'afterward', 'princess', 'dorothy', 'of', 'oz', ',', 'an', 'humble', 'writer', 'in', 'the', 'united', 'states', 'of', 'america', 'was', 'once', 'appointed', 'royal', 'historian', 'of', 'oz', ',', 'with', 'the', 'privilege', 'of', 'writing', 'the', 'chronicle', 'of', 'that', 'wonderful', 'fairyland', '.'] ['produced', 'by', 'anthony', 'matonac', 'and', 'paul', 'selkirk', 'the', 'patchwork', 'girl', 'of', 'oz', 'by', 'l.', 'frank', 'baum', 'affectionately', 'dedicated', 'to', 'my', 'young', 'friend', 'sumner', 'hamilton', 'britton', 'of', 'chicago', 'prologue', 'through', 'the', 'kindness', 'of', 'dorothy', 'gale', 'of', 'kansas', ',', 'afterward', 'princess', 'dorothy'] [606, 30, 5995, 13700, 8, 1387, 8547, 4, 469, 221, 7, 236, 30, 4423, 2747, 3685, 7768, 3044, 11, 69, 339, 498, 26737, 4041, 26738, 7, 2055, 26739, 171, 4, 4042, 7, 176, 5616, 7, 2179, 5, 5996, 526, 176, 7, 236, 5, 41, 6490, 1301, 9, 4, 111, 112, 7, 678, 15, 298, 1523, 321, 3271, 7, 236, 5, 23, 4, 1388, 7, 1067, 4, 11053, 7, 19, 1086, 3160, 6] [606, 30, 5995, 13700, 8, 1387, 8547, 4, 469, 221, 7, 236, 30, 4423, 2747, 3685, 7768, 3044, 11, 69, 339, 498, 26737, 4041, 26738, 7, 2055, 26739, 171, 4, 4042, 7, 176, 5616, 7, 2179, 5, 5996, 526, 176]\n"
     ]
    }
   ],
   "source": [
    "x = comment_set.pos_sentences[0]\n",
    "print(x.tokens, x.padded_tokens, x.token_ids, x.padded_tokens_ids)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "x = wiki_set.pos_sentences[0]\n",
    "print(x.tokens, x.padded_tokens, x.token_ids, x.padded_tokens_ids)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "x = merge_set.pos_sentences[0]\n",
    "print(x.tokens, x.padded_tokens, x.token_ids, x.padded_tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frac = 0.2\n",
    "train_frac = 0.8\n",
    "\n",
    "comment_set.divide_test_train(test_frac, train_frac)\n",
    "wiki_set.divide_test_train(test_frac, train_frac)\n",
    "merge_set.divide_test_train(test_frac, train_frac)\n",
    "\n",
    "comment_set.prep_sets()\n",
    "wiki_set.prep_sets()\n",
    "merge_set.prep_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_arr = [(\"ass and titties\", KID_INAPPROPRIATE),\n",
    "(\"fuck the police\", KID_INAPPROPRIATE),\n",
    "(\"cats are nice\", KID_APPROPRIATE),\n",
    "(\"one day I saw a horse on a hill and I liked it\", KID_APPROPRIATE),\n",
    "(\"mrs. tayler is a whore\", KID_INAPPROPRIATE),\n",
    "(\"I'd like to take you someplace nice and quiet\", KID_INAPPROPRIATE),\n",
    "(\"You gay ? Proceeds to chase him that's how bullying works\", KID_INAPPROPRIATE),\n",
    "(\"transfer responsibility dad temperature earn voter impossible radiation.\", KID_APPROPRIATE),\n",
    "(\"JADE CHYNOWETH AND JOSH KILLACKY LOVETEAM PLS\", KID_APPROPRIATE),\n",
    "(\"Lol, looks like the same generic crap you come to expect from these types of shows.  Guess it doesn't get old for some people.\", KID_INAPPROPRIATE),\n",
    "(\"Disney sold the rights to this?\", KID_APPROPRIATE),\n",
    "(\"Only step up 1 and 2 were good. All of the other ones have just been corny.\", KID_APPROPRIATE),\n",
    "(\"i rather not call it step up, its another level with different camera techniques and its so Channing Tatum style\", KID_APPROPRIATE),\n",
    "(\"I love the series its very good, im waiting for 2 one\", KID_APPROPRIATE),\n",
    "(\"GREAT\", KID_APPROPRIATE),\n",
    "(\"Job negotiate set alternative little introduction apparent crazy proper used care free.\", KID_APPROPRIATE),\n",
    "(\"Oxygen identify member dependent translate else card might handful.\", KID_APPROPRIATE),\n",
    "(\"Release accompany pole general something widely fly cup detective personnel.\", KID_APPROPRIATE),\n",
    "(\"Silly largely obstacle warrior charge flavor diabetes medal.\", KID_APPROPRIATE),\n",
    "(\"We need moose back\", KID_APPROPRIATE),\n",
    "(\"Exciting time to be alive bro watching our Prez Trump vs. the DeepState/Swamp that's the deal profs to him.\", KID_APPROPRIATE),\n",
    "(\"Idk if I want to watch it or not : ^/\", KID_APPROPRIATE),\n",
    "(\"Maximum testing blanket absolutely shock until actress sex liability.\", KID_INAPPROPRIATE),\n",
    "(\"3 was the best.\", KID_APPROPRIATE),\n",
    "(\"I knew they would have a all men gay dance group\", KID_INAPPROPRIATE),\n",
    "(\"Same old garbage; I'm sick of it !\", KID_APPROPRIATE),\n",
    "(\"GAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYYYYYYYYYYYYYYYYYYYYYYYYY\",KID_INAPPROPRIATE),\n",
    "(\"Millenials = The worst generation\",KID_APPROPRIATE),\n",
    "(\"that opening statement is the most overused sentence i've seen\",KID_APPROPRIATE),\n",
    "(\"Pan spokesman via guard campaign characteristic movement expert attend garage.\",KID_APPROPRIATE),\n",
    "(\"ayyy https://youtu.be/wZwyQZDfyHY\",KID_APPROPRIATE),\n",
    "(\"WELP....the end of the step up franchise met a very disappointing end. Jesus this looks worse than most dumpster fires Ive seen\",KID_APPROPRIATE),\n",
    "(\"blacks that look white and wiggas.  yeah, looks great. (not!)\",KID_INAPPROPRIATE),\n",
    "(\"Rich And Diverse artistry- Advocating White Genocide. How is enrichment going on in South Africa?\",KID_INAPPROPRIATE),\n",
    "(\"step up <3\",KID_APPROPRIATE),\n",
    "(\"When is episode 5 coming out\",KID_APPROPRIATE),\n",
    "(\"This series is actually pretty good...watched it and was so happy I gave it a shot; I loved it! The storyline is actually stronger than I expected, and especially where they left off this first season, I'm so excited for what's to come; I hope they keep up the good work and produce even more (authentic stuff next season.\",KID_APPROPRIATE),\n",
    "(\"https://vk.com/club162181675 вступайте\",KID_APPROPRIATE),\n",
    "(\"lunch argument across expansion free govern healthy afternoon.\",KID_APPROPRIATE),\n",
    "(\"Modern day version of Fame!\",KID_APPROPRIATE),\n",
    "(\"Neyo!\",KID_APPROPRIATE),\n",
    "(\"I wasn't expecting this from YouTube but it's really good\",KID_APPROPRIATE),\n",
    "(\"The\",KID_APPROPRIATE),\n",
    "(\"what garbage i this want to be step up is more like stomp the yard\",KID_APPROPRIATE),\n",
    "(\"if you're actually thinking of watching this show... do it. i recommend it. it's so much more than what we ever got to see from step up or any dance movies at all. so much diversity & so many topics it's talking about. i was surprised at how much i actually enjoyed the show & how bad i need a (second season. just give it a try since the first 4 episodes are for free on here, i don't think y'all are gonna regret it\",KID_APPROPRIATE),\n",
    "(\"Aren’t u guys releasing more seasons and episodes??? we still need more of ‘em\",KID_APPROPRIATE),\n",
    "(\"Have Chris brown in the step up movies it’ll be dope\",KID_APPROPRIATE),\n",
    "(\"http://hikmatblogs.blogspot.com/2018/02/10-ways-to-improve-your-finances-today.html\",KID_APPROPRIATE),\n",
    "(\"No Moose no Step Up\",KID_APPROPRIATE),\n",
    "(\"Let me guess?? Suburban girl trying to prove her gangsta and most of the black men are gay.... WHACK!!!\",KID_INAPPROPRIATE),\n",
    "(\"What, no moose cameo ?\",KID_APPROPRIATE),\n",
    "(\"trash.\",KID_APPROPRIATE),\n",
    "(\"Why does everything have to be so gay  won't be watching this sick of all our black men in tv series being gay smh\",KID_INAPPROPRIATE),\n",
    "(\"Who's that girl on the trailer cover?\",KID_APPROPRIATE),\n",
    "(\"Atlanta is a n!66er filled cesspool.. i can't figure out why parents stopped teaching their children white from wrong.. why today's youth wants grey babies.. why ruin family pictures with those half n!66e kinky haired flat nosed kids..  why these fathers aren't teaching their daughters to stay off the chimpdicks.. i mean, for real, how low does your self esteem have to be to want to share your beautiful young white body with these savage nigloyds... i'm sorry your fathers have failed you girls.. I taught mine white from wrong.. my grandkids will be bright white with blonde hair and blue eyes.. not gray with kinky rugs and flat noses.. just the idea of beautiful young white girls with sweaty, greasy n!66ers up on them just makes me wanna puke.. parents.. it's time to start teaching some basic family values again.. not the kardashian kind where they just keep getting impregnated by stray chimps.. it's just disgusting... let's clean up and keep the white blood lines free from contaminants..  you young white girls... just say no.. find a decent white boy to make babies with.. you can't tell me that a n!66er is all you can get..  well, maybe the fat ugly girls.. but even you can do better.. find a fat ugly white guy..  give him that poontang .. not these foot stomping, drum banging monkeys .. talentless rappers...  all wanna pretend they are in the music business.. rap isn't music.. any tar baby can get a drum machine and call it a beat lab.. then they steal a macbook from a white girl.. and start their career as the white girl's baby daddy.. she says he's a musician.. but he's really just another worthless n166er  he will keep her on welfare.. just a waste.. february is black history month.. the other 11 is caucasian (history months.. we will celebrate by taking our women back..\",KID_INAPPROPRIATE),\n",
    "(\"i run them streetsXD\",KID_APPROPRIATE),\n",
    "(\"my answer is....hell noXD\",KID_APPROPRIATE),\n",
    "(\"Sail star diamond rate working love pond size that monument prevention celebrate.\",KID_APPROPRIATE),\n",
    "(\"So is Kevin Bacon they're Jesus or something?\",KID_APPROPRIATE),\n",
    "(\"Can someone please give me the name of the girl on the thumbnail please ?????\",KID_APPROPRIATE),\n",
    "(\"I would rather be prison raped than watch 5seconds of this garbage\",KID_INAPPROPRIATE),\n",
    "(\"Prosecution tool before endless visit dump shake sake remarkable hurt safe public.\",KID_APPROPRIATE),\n",
    "(\"total grief slave esyvbp question fourth fun basic fly sacrifice reply.\",KID_INAPPROPRIATE),\n",
    "(\"As she named the Empress, Anna Pávlovna’s face suddenly assumed an expression of profound and sincere devotion and respect mingled with sadness, and this occurred every time she mentioned her illustrious patroness. \", KID_INAPPROPRIATE),\n",
    "(\"She added that Her Majesty had deigned to show Baron Funke beaucoup d’estime, and again her face clouded over with sadness.\", KID_INAPPROPRIATE),\n",
    "(\"The prince was silent and looked indifferent. \", KID_APPROPRIATE),\n",
    "(\"But, with the womanly and courtierlike quickness and tact habitual to her, Anna Pávlovna wished both to rebuke him (for daring to speak as he had done of a ma \", KID_INAPPROPRIATE),\n",
    "(\"recommended to the Empress) and at the same time to console him, so she said \", KID_INAPPROPRIATE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models;\n",
    "\n",
    "# ts = comment_set\n",
    "ts = wiki_set\n",
    "\n",
    "def train_and_test_bow(ts):\n",
    "    start_time = time.time()\n",
    "    x, ns, y, ext_y = ts.trains\n",
    "    \n",
    "    test_set = TrainingSet()\n",
    "\n",
    "    test_set.load_test_arr(test_arr,tokenizer)\n",
    "    test_set.set_vocab(ts.vocab)\n",
    "    test_set.prep_sents(PAD_LEN)\n",
    "    test_set.prep_sets()\n",
    "\n",
    "    # Training params\n",
    "    batch_size = 64\n",
    "    model_params = dict(V=ts.vocab, embed_dim=70, hidden_dims=[75,50,25], num_classes=len(labels),\n",
    "                        encoder_type='bow',\n",
    "                        lr=0.1, optimizer='adagrad', beta=0.01)\n",
    "    model_fn = models.classifier_model_fn\n",
    "\n",
    "    # training\n",
    "    total_batches = 0\n",
    "    total_examples = 0\n",
    "    total_loss = 0\n",
    "    loss_ema = np.log(2)  # track exponential-moving-average of loss\n",
    "    ema_decay = np.exp(-1/10)  # decay parameter for moving average = np.exp(-1/history_length)\n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "        ##\n",
    "        # Construct the graph here. No session.run calls - just wiring up Tensors.\n",
    "        ##\n",
    "        # Add placeholders so we can feed in data.\n",
    "        x_ph_  = tf.placeholder(tf.int32, shape=[None, x.shape[1]])  # [batch_size, max_len]\n",
    "        ns_ph_ = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "        y_ph_  = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "\n",
    "        # Construct the graph using model_fn\n",
    "        features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "        estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                                  params=model_params)\n",
    "        loss_     = estimator_spec.loss\n",
    "        train_op_ = estimator_spec.train_op\n",
    "\n",
    "        ##\n",
    "        # Done constructing the graph, now we can make session.run calls.\n",
    "        ##\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    checkpoint_dir = \"/tmp/tf_bow_sst_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    if os.path.isdir(checkpoint_dir):\n",
    "        shutil.rmtree(checkpoint_dir)\n",
    "    # Write vocabulary to file, so TensorBoard can label embeddings.\n",
    "    # creates checkpoint_dir/projector_config.pbtxt and checkpoint_dir/metadata.tsv\n",
    "    ts.vocab.write_projector_config(checkpoint_dir, \"Encoder/Embedding_Layer/W_embed\")\n",
    "\n",
    "    model = tf.estimator.Estimator(model_fn=models.classifier_model_fn, \n",
    "                                   params=model_params,\n",
    "                                   model_dir=checkpoint_dir)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"To view training (once it starts), run:\\n\")\n",
    "    print(\"    tensorboard --logdir='{:s}' --port 6006\".format(checkpoint_dir))\n",
    "    print(\"\\nThen in your browser, open: http://localhost:6006\")\n",
    "    \n",
    "    # Training params, just used in this cell for the input_fn-s\n",
    "    train_params = dict(batch_size=32, total_epochs=20, eval_every=10)\n",
    "    assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "    # Construct and train the model, saving checkpoints to the directory above.\n",
    "    # Input function for training set batches\n",
    "    # Do 'eval_every' epochs at once, followed by evaluating on the dev set.\n",
    "    # NOTE: use patch_numpy_io.numpy_input_fn instead of tf.estimator.inputs.numpy_input_fn\n",
    "    train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                        x={\"ids\": x, \"ns\": ns}, y=y,\n",
    "                        batch_size=train_params['batch_size'], \n",
    "                        num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                     )\n",
    "    \n",
    "    # Input function for dev set batches. As above, but:\n",
    "    # - Don't randomize order\n",
    "    # - Iterate exactly once (one epoch)\n",
    "    dev_x, dev_ns, dev_y, dev_y_ext = ts.devs\n",
    "    dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                        x={\"ids\": dev_x, \"ns\": dev_ns}, y=dev_y,\n",
    "                        batch_size=128, num_epochs=1, shuffle=False\n",
    "                    )\n",
    "\n",
    "    for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "        # Train for a few epochs, then evaluate on dev\n",
    "        model.train(input_fn=train_input_fn)\n",
    "        eval_metrics = model.evaluate(input_fn=dev_input_fn, name=\"dev\")\n",
    "        \n",
    "    sd_test_x, sd_test_ns, sd_test_y, _ = ts.sds\n",
    "    test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                        x={\"ids\": sd_test_x, \"ns\": sd_test_ns}, y=sd_test_y,\n",
    "                        batch_size=128, num_epochs=1, shuffle=False\n",
    "                    )\n",
    "\n",
    "    same_domain_eval = model.evaluate(input_fn=test_input_fn, name=\"eval\")\n",
    "\n",
    "\n",
    "    print(\"Accuracy on same domain test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "\n",
    "    \n",
    "    test_x, test_ns, test_y, test_y_ext = test_set.ext_tests\n",
    "    test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                        x={\"ids\": test_x, \"ns\": test_ns}, y=test_y,\n",
    "                        batch_size=128, num_epochs=1, shuffle=False\n",
    "                    )\n",
    "\n",
    "    new_domain_eval = model.evaluate(input_fn=test_input_fn, name=\"eval\")\n",
    "    print(\"Accuracy on generalized test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score\n",
    "    predictions = list(model.predict(test_input_fn))  # list of dicts\n",
    "    y_pred = [p['max'] for p in predictions]\n",
    "    acc = accuracy_score(y_pred, test_y)\n",
    "    print(\"Accuracy on test set: {:.02%}\".format(acc))\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    return same_domain_eval, new_domain_eval, predictions, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (37,306 words) written to '/tmp/tf_bow_sst_20180416-0448/metadata.tsv'\n",
      "Projector config written to /tmp/tf_bow_sst_20180416-0448/projector_config.pbtxt\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_bow_sst_20180416-0448', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f351fb1feb8>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/tf_bow_sst_20180416-0448' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tf_bow_sst_20180416-0448/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.8093283, step = 1\n",
      "INFO:tensorflow:global_step/sec: 80.6839\n",
      "INFO:tensorflow:loss = 0.8965368, step = 101 (1.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.6412\n",
      "INFO:tensorflow:loss = 0.54733026, step = 201 (1.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.956\n",
      "INFO:tensorflow:loss = 0.3656832, step = 301 (1.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.3261\n",
      "INFO:tensorflow:loss = 0.25807446, step = 401 (1.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.2501\n",
      "INFO:tensorflow:loss = 0.24093115, step = 501 (1.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.0414\n",
      "INFO:tensorflow:loss = 0.2088424, step = 601 (1.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.3414\n",
      "INFO:tensorflow:loss = 0.19392096, step = 701 (1.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.4833\n",
      "INFO:tensorflow:loss = 0.15194403, step = 801 (1.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.2923\n",
      "INFO:tensorflow:loss = 0.12703425, step = 901 (1.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.8041\n",
      "INFO:tensorflow:loss = 0.13423517, step = 1001 (1.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.8984\n",
      "INFO:tensorflow:loss = 0.30125338, step = 1101 (1.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.0988\n",
      "INFO:tensorflow:loss = 0.09624322, step = 1201 (1.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.6508\n",
      "INFO:tensorflow:loss = 0.12577245, step = 1301 (1.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.7204\n",
      "INFO:tensorflow:loss = 0.10539977, step = 1401 (1.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6948\n",
      "INFO:tensorflow:loss = 0.105887696, step = 1501 (1.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.2122\n",
      "INFO:tensorflow:loss = 0.10933939, step = 1601 (1.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.3341\n",
      "INFO:tensorflow:loss = 0.10088517, step = 1701 (1.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.8203\n",
      "INFO:tensorflow:loss = 0.102343045, step = 1801 (1.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.0361\n",
      "INFO:tensorflow:loss = 0.10602873, step = 1901 (1.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.7847\n",
      "INFO:tensorflow:loss = 0.09553995, step = 2001 (1.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.683\n",
      "INFO:tensorflow:loss = 0.09733127, step = 2101 (1.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3683\n",
      "INFO:tensorflow:loss = 0.09849049, step = 2201 (1.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.0653\n",
      "INFO:tensorflow:loss = 0.094778, step = 2301 (1.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.3058\n",
      "INFO:tensorflow:loss = 0.1068487, step = 2401 (1.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.3395\n",
      "INFO:tensorflow:loss = 0.09641189, step = 2501 (1.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.0618\n",
      "INFO:tensorflow:loss = 0.09103911, step = 2601 (1.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.6553\n",
      "INFO:tensorflow:loss = 0.10405115, step = 2701 (1.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.67\n",
      "INFO:tensorflow:loss = 0.09727701, step = 2801 (1.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.2275\n",
      "INFO:tensorflow:loss = 0.09592256, step = 2901 (1.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.9722\n",
      "INFO:tensorflow:loss = 0.109100476, step = 3001 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.2241\n",
      "INFO:tensorflow:loss = 0.11598571, step = 3101 (1.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.1658\n",
      "INFO:tensorflow:loss = 0.09458554, step = 3201 (1.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.0578\n",
      "INFO:tensorflow:loss = 0.10551381, step = 3301 (1.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.1688\n",
      "INFO:tensorflow:loss = 0.087756455, step = 3401 (1.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.8524\n",
      "INFO:tensorflow:loss = 0.09439181, step = 3501 (1.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.4622\n",
      "INFO:tensorflow:loss = 0.100594684, step = 3601 (1.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.9272\n",
      "INFO:tensorflow:loss = 0.09054562, step = 3701 (1.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9728\n",
      "INFO:tensorflow:loss = 0.094629355, step = 3801 (1.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.9676\n",
      "INFO:tensorflow:loss = 0.0866057, step = 3901 (1.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.6931\n",
      "INFO:tensorflow:loss = 0.08897118, step = 4001 (1.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.4261\n",
      "INFO:tensorflow:loss = 0.10625476, step = 4101 (1.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.109\n",
      "INFO:tensorflow:loss = 0.08992001, step = 4201 (1.264 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.8183\n",
      "INFO:tensorflow:loss = 0.10080573, step = 4301 (1.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.3319\n",
      "INFO:tensorflow:loss = 0.08824732, step = 4401 (1.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.3334\n",
      "INFO:tensorflow:loss = 0.0979776, step = 4501 (1.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.8464\n",
      "INFO:tensorflow:loss = 0.09100324, step = 4601 (1.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.9632\n",
      "INFO:tensorflow:loss = 0.09034618, step = 4701 (1.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9275\n",
      "INFO:tensorflow:loss = 0.0867877, step = 4801 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.7945\n",
      "INFO:tensorflow:loss = 0.09070264, step = 4901 (1.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.407\n",
      "INFO:tensorflow:loss = 0.08783151, step = 5001 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.8895\n",
      "INFO:tensorflow:loss = 0.093509875, step = 5101 (1.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.6766\n",
      "INFO:tensorflow:loss = 0.0882594, step = 5201 (1.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.1194\n",
      "INFO:tensorflow:loss = 0.08677553, step = 5301 (1.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.8474\n",
      "INFO:tensorflow:loss = 0.08793653, step = 5401 (1.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.1752\n",
      "INFO:tensorflow:loss = 0.085567385, step = 5501 (1.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.408\n",
      "INFO:tensorflow:loss = 0.087338574, step = 5601 (1.309 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.5543\n",
      "INFO:tensorflow:loss = 0.09152616, step = 5701 (1.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.4209\n",
      "INFO:tensorflow:loss = 0.09094952, step = 5801 (1.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.6643\n",
      "INFO:tensorflow:loss = 0.15224415, step = 5901 (1.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.8189\n",
      "INFO:tensorflow:loss = 0.0918806, step = 6001 (1.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.9682\n",
      "INFO:tensorflow:loss = 0.11853442, step = 6101 (1.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.9199\n",
      "INFO:tensorflow:loss = 0.09380732, step = 6201 (1.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.2487\n",
      "INFO:tensorflow:loss = 0.088552445, step = 6301 (1.278 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.334\n",
      "INFO:tensorflow:loss = 0.08550795, step = 6401 (1.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.0402\n",
      "INFO:tensorflow:loss = 0.09296172, step = 6501 (1.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.5369\n",
      "INFO:tensorflow:loss = 0.09204641, step = 6601 (1.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.009\n",
      "INFO:tensorflow:loss = 0.10895687, step = 6701 (1.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.4175\n",
      "INFO:tensorflow:loss = 0.08482543, step = 6801 (1.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.3984\n",
      "INFO:tensorflow:loss = 0.08913446, step = 6901 (1.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.7833\n",
      "INFO:tensorflow:loss = 0.088591635, step = 7001 (1.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.7909\n",
      "INFO:tensorflow:loss = 0.089421205, step = 7101 (1.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.5133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.0893367, step = 7201 (1.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.9257\n",
      "INFO:tensorflow:loss = 0.15086398, step = 7301 (1.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.4298\n",
      "INFO:tensorflow:loss = 0.08766577, step = 7401 (1.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.6337\n",
      "INFO:tensorflow:loss = 0.10899597, step = 7501 (1.305 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.7273\n",
      "INFO:tensorflow:loss = 0.35899395, step = 7601 (1.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2881\n",
      "INFO:tensorflow:loss = 0.08509956, step = 7701 (1.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.2779\n",
      "INFO:tensorflow:loss = 0.09705454, step = 7801 (1.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.5617\n",
      "INFO:tensorflow:loss = 0.09210501, step = 7901 (1.197 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8000 into /tmp/tf_bow_sst_20180416-0448/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.08727206.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-04:50:45\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180416-0448/model.ckpt-8000\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-04:50:46\n",
      "INFO:tensorflow:Saving dict for global step 8000: accuracy = 0.9865625, cross_entropy_loss = 0.053479057, global_step = 8000, loss = 0.17477964\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180416-0448/model.ckpt-8000\n",
      "INFO:tensorflow:Saving checkpoints for 8001 into /tmp/tf_bow_sst_20180416-0448/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.35431135, step = 8001\n",
      "INFO:tensorflow:global_step/sec: 74.2622\n",
      "INFO:tensorflow:loss = 0.08661364, step = 8101 (1.350 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.4392\n",
      "INFO:tensorflow:loss = 0.09470055, step = 8201 (1.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.7859\n",
      "INFO:tensorflow:loss = 0.08528322, step = 8301 (1.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.138\n",
      "INFO:tensorflow:loss = 0.09093307, step = 8401 (1.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.2435\n",
      "INFO:tensorflow:loss = 0.08568396, step = 8501 (1.262 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.3795\n",
      "INFO:tensorflow:loss = 0.09103209, step = 8601 (1.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.2124\n",
      "INFO:tensorflow:loss = 0.087999985, step = 8701 (1.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.2625\n",
      "INFO:tensorflow:loss = 0.09323679, step = 8801 (1.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.2387\n",
      "INFO:tensorflow:loss = 0.10169702, step = 8901 (1.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.8948\n",
      "INFO:tensorflow:loss = 0.090545066, step = 9001 (1.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.2058\n",
      "INFO:tensorflow:loss = 0.11418751, step = 9101 (1.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.4996\n",
      "INFO:tensorflow:loss = 0.08089346, step = 9201 (1.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.7651\n",
      "INFO:tensorflow:loss = 0.09746726, step = 9301 (1.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.8981\n",
      "INFO:tensorflow:loss = 0.08560319, step = 9401 (1.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.3614\n",
      "INFO:tensorflow:loss = 0.089866124, step = 9501 (1.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.5639\n",
      "INFO:tensorflow:loss = 0.08743517, step = 9601 (1.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.9489\n",
      "INFO:tensorflow:loss = 0.09359301, step = 9701 (1.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.9104\n",
      "INFO:tensorflow:loss = 0.08623025, step = 9801 (1.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.0349\n",
      "INFO:tensorflow:loss = 0.09339428, step = 9901 (1.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.1726\n",
      "INFO:tensorflow:loss = 0.086245984, step = 10001 (1.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.0933\n",
      "INFO:tensorflow:loss = 0.09291658, step = 10101 (1.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.6384\n",
      "INFO:tensorflow:loss = 0.0872863, step = 10201 (1.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.9656\n",
      "INFO:tensorflow:loss = 0.0875073, step = 10301 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.051\n",
      "INFO:tensorflow:loss = 0.09095743, step = 10401 (1.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.2541\n",
      "INFO:tensorflow:loss = 0.08550354, step = 10501 (1.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.5991\n",
      "INFO:tensorflow:loss = 0.082684755, step = 10601 (1.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.9381\n",
      "INFO:tensorflow:loss = 0.094564304, step = 10701 (1.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.1283\n",
      "INFO:tensorflow:loss = 0.09018354, step = 10801 (1.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.9718\n",
      "INFO:tensorflow:loss = 0.08776535, step = 10901 (1.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.6851\n",
      "INFO:tensorflow:loss = 0.096756145, step = 11001 (1.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.5368\n",
      "INFO:tensorflow:loss = 0.09488423, step = 11101 (1.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9214\n",
      "INFO:tensorflow:loss = 0.099819176, step = 11201 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.0471\n",
      "INFO:tensorflow:loss = 0.09189411, step = 11301 (1.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.8304\n",
      "INFO:tensorflow:loss = 0.08294875, step = 11401 (1.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.7237\n",
      "INFO:tensorflow:loss = 0.08682274, step = 11501 (1.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.9004\n",
      "INFO:tensorflow:loss = 0.091773465, step = 11601 (1.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.1704\n",
      "INFO:tensorflow:loss = 0.08312595, step = 11701 (1.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.6319\n",
      "INFO:tensorflow:loss = 0.08783535, step = 11801 (1.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.8715\n",
      "INFO:tensorflow:loss = 0.08152229, step = 11901 (1.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.9848\n",
      "INFO:tensorflow:loss = 0.08407453, step = 12001 (1.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.6879\n",
      "INFO:tensorflow:loss = 0.09105733, step = 12101 (1.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.2673\n",
      "INFO:tensorflow:loss = 0.08674468, step = 12201 (1.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.6621\n",
      "INFO:tensorflow:loss = 0.09411765, step = 12301 (1.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.3262\n",
      "INFO:tensorflow:loss = 0.08444178, step = 12401 (1.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.0854\n",
      "INFO:tensorflow:loss = 0.09027855, step = 12501 (1.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.1703\n",
      "INFO:tensorflow:loss = 0.08518241, step = 12601 (1.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.0502\n",
      "INFO:tensorflow:loss = 0.087588765, step = 12701 (1.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.3936\n",
      "INFO:tensorflow:loss = 0.08377855, step = 12801 (1.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.3329\n",
      "INFO:tensorflow:loss = 0.08587271, step = 12901 (1.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.1688\n",
      "INFO:tensorflow:loss = 0.08403504, step = 13001 (1.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.8219\n",
      "INFO:tensorflow:loss = 0.08909103, step = 13101 (1.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.5966\n",
      "INFO:tensorflow:loss = 0.08446719, step = 13201 (1.306 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.954\n",
      "INFO:tensorflow:loss = 0.08338452, step = 13301 (1.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.2268\n",
      "INFO:tensorflow:loss = 0.08493298, step = 13401 (1.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.5717\n",
      "INFO:tensorflow:loss = 0.08269742, step = 13501 (1.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.6931\n",
      "INFO:tensorflow:loss = 0.08369703, step = 13601 (1.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.1264\n",
      "INFO:tensorflow:loss = 0.08875732, step = 13701 (1.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.1805\n",
      "INFO:tensorflow:loss = 0.089218594, step = 13801 (1.279 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.4311\n",
      "INFO:tensorflow:loss = 0.14270169, step = 13901 (1.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.5131\n",
      "INFO:tensorflow:loss = 0.086100586, step = 14001 (1.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.6709\n",
      "INFO:tensorflow:loss = 0.107226714, step = 14101 (1.321 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.9174\n",
      "INFO:tensorflow:loss = 0.09054908, step = 14201 (1.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.9898\n",
      "INFO:tensorflow:loss = 0.08464546, step = 14301 (1.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.2354\n",
      "INFO:tensorflow:loss = 0.081595026, step = 14401 (1.314 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.5412\n",
      "INFO:tensorflow:loss = 0.09171355, step = 14501 (1.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.4868\n",
      "INFO:tensorflow:loss = 0.08842288, step = 14601 (1.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.0324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.1033243, step = 14701 (1.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.7589\n",
      "INFO:tensorflow:loss = 0.08166809, step = 14801 (1.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.0556\n",
      "INFO:tensorflow:loss = 0.08565057, step = 14901 (1.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.1504\n",
      "INFO:tensorflow:loss = 0.08530132, step = 15001 (1.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.8507\n",
      "INFO:tensorflow:loss = 0.086081654, step = 15101 (1.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.1197\n",
      "INFO:tensorflow:loss = 0.08591641, step = 15201 (1.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.6684\n",
      "INFO:tensorflow:loss = 0.13979164, step = 15301 (1.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.7925\n",
      "INFO:tensorflow:loss = 0.084333405, step = 15401 (1.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.1421\n",
      "INFO:tensorflow:loss = 0.1059624, step = 15501 (1.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.838\n",
      "INFO:tensorflow:loss = 0.36588824, step = 15601 (1.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.2912\n",
      "INFO:tensorflow:loss = 0.0820275, step = 15701 (1.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0705\n",
      "INFO:tensorflow:loss = 0.09442714, step = 15801 (1.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.8809\n",
      "INFO:tensorflow:loss = 0.09205532, step = 15901 (1.178 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 16000 into /tmp/tf_bow_sst_20180416-0448/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.083786406.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-04:52:30\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180416-0448/model.ckpt-16000\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-04:52:31\n",
      "INFO:tensorflow:Saving dict for global step 16000: accuracy = 0.98625, cross_entropy_loss = 0.05627717, global_step = 16000, loss = 0.17312458\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-04:52:33\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180416-0448/model.ckpt-16000\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-04:52:34\n",
      "INFO:tensorflow:Saving dict for global step 16000: accuracy = 0.987625, cross_entropy_loss = 0.056523755, global_step = 16000, loss = 0.17352043\n",
      "Accuracy on same domain test set: 98.62%\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-04:52:37\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180416-0448/model.ckpt-16000\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-04:52:37\n",
      "INFO:tensorflow:Saving dict for global step 16000: accuracy = 0.44117647, cross_entropy_loss = 1.5629603, global_step = 16000, loss = 1.642161\n",
      "Accuracy on generalized test set: 98.62%\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tf_bow_sst_20180416-0448/model.ckpt-16000\n",
      "Accuracy on test set: 44.12%\n",
      "Vocabulary (40,075 words) written to '/tmp/tf_bow_sst_20180416-0452/metadata.tsv'\n",
      "Projector config written to /tmp/tf_bow_sst_20180416-0452/projector_config.pbtxt\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tf_bow_sst_20180416-0452', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f351d1c0550>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "To view training (once it starts), run:\n",
      "\n",
      "    tensorboard --logdir='/tmp/tf_bow_sst_20180416-0452' --port 6006\n",
      "\n",
      "Then in your browser, open: http://localhost:6006\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tf_bow_sst_20180416-0452/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.8163244, step = 1\n",
      "INFO:tensorflow:global_step/sec: 79.6626\n",
      "INFO:tensorflow:loss = 1.4285235, step = 101 (1.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.0575\n",
      "INFO:tensorflow:loss = 1.1556145, step = 201 (1.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.8185\n",
      "INFO:tensorflow:loss = 1.0130937, step = 301 (1.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.8428\n",
      "INFO:tensorflow:loss = 0.958019, step = 401 (1.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.3386\n",
      "INFO:tensorflow:loss = 0.95234066, step = 501 (1.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.1355\n",
      "INFO:tensorflow:loss = 0.88120514, step = 601 (1.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.9781\n",
      "INFO:tensorflow:loss = 0.87197423, step = 701 (1.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.0883\n",
      "INFO:tensorflow:loss = 0.8320147, step = 801 (1.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.5477\n",
      "INFO:tensorflow:loss = 0.7538681, step = 901 (1.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.2997\n",
      "INFO:tensorflow:loss = 0.7825693, step = 1001 (1.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.8623\n",
      "INFO:tensorflow:loss = 0.79908204, step = 1101 (1.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.3559\n",
      "INFO:tensorflow:loss = 0.73924464, step = 1201 (1.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.3148\n",
      "INFO:tensorflow:loss = 0.74994206, step = 1301 (1.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.855\n",
      "INFO:tensorflow:loss = 0.72255087, step = 1401 (1.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.9011\n",
      "INFO:tensorflow:loss = 0.78874826, step = 1501 (1.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.6659\n",
      "INFO:tensorflow:loss = 0.6758838, step = 1601 (1.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.68\n",
      "INFO:tensorflow:loss = 0.76087517, step = 1701 (1.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.1591\n",
      "INFO:tensorflow:loss = 0.7704773, step = 1801 (1.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.2621\n",
      "INFO:tensorflow:loss = 0.67908245, step = 1901 (1.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.5396\n",
      "INFO:tensorflow:loss = 0.7142614, step = 2001 (1.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.9615\n",
      "INFO:tensorflow:loss = 0.7501181, step = 2101 (1.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.6817\n",
      "INFO:tensorflow:loss = 0.69848853, step = 2201 (1.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.6736\n",
      "INFO:tensorflow:loss = 0.65628433, step = 2301 (1.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.5841\n",
      "INFO:tensorflow:loss = 0.70671576, step = 2401 (1.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0434\n",
      "INFO:tensorflow:loss = 0.63636786, step = 2501 (1.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.0626\n",
      "INFO:tensorflow:loss = 0.68637186, step = 2601 (1.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.2782\n",
      "INFO:tensorflow:loss = 0.74397254, step = 2701 (1.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.3489\n",
      "INFO:tensorflow:loss = 0.6677877, step = 2801 (1.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.6138\n",
      "INFO:tensorflow:loss = 0.61487097, step = 2901 (1.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.1685\n",
      "INFO:tensorflow:loss = 0.67153513, step = 3001 (1.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.2529\n",
      "INFO:tensorflow:loss = 0.66172016, step = 3101 (1.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.939\n",
      "INFO:tensorflow:loss = 0.6584766, step = 3201 (1.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.5341\n",
      "INFO:tensorflow:loss = 0.61339414, step = 3301 (1.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.8529\n",
      "INFO:tensorflow:loss = 0.7106339, step = 3401 (1.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.9823\n",
      "INFO:tensorflow:loss = 0.70240694, step = 3501 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.4342\n",
      "INFO:tensorflow:loss = 0.6608486, step = 3601 (1.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.1474\n",
      "INFO:tensorflow:loss = 0.647459, step = 3701 (1.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.5201\n",
      "INFO:tensorflow:loss = 0.57818323, step = 3801 (1.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.5638\n",
      "INFO:tensorflow:loss = 0.7689271, step = 3901 (1.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.841\n",
      "INFO:tensorflow:loss = 0.72458434, step = 4001 (1.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.1144\n",
      "INFO:tensorflow:loss = 0.7015673, step = 4101 (1.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.8704\n",
      "INFO:tensorflow:loss = 0.6128246, step = 4201 (1.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.225\n",
      "INFO:tensorflow:loss = 0.42873693, step = 4301 (1.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.7481\n",
      "INFO:tensorflow:loss = 0.50516844, step = 4401 (1.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.4941\n",
      "INFO:tensorflow:loss = 0.6877443, step = 4501 (1.170 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 87.093\n",
      "INFO:tensorflow:loss = 0.62364393, step = 4601 (1.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.3404\n",
      "INFO:tensorflow:loss = 0.62300575, step = 4701 (1.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.3634\n",
      "INFO:tensorflow:loss = 0.5238902, step = 4801 (1.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0231\n",
      "INFO:tensorflow:loss = 0.70848477, step = 4901 (1.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.1274\n",
      "INFO:tensorflow:loss = 0.49282494, step = 5001 (1.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.1528\n",
      "INFO:tensorflow:loss = 0.43007684, step = 5101 (1.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.0561\n",
      "INFO:tensorflow:loss = 0.42318267, step = 5201 (1.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.4084\n",
      "INFO:tensorflow:loss = 0.441622, step = 5301 (1.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0597\n",
      "INFO:tensorflow:loss = 0.679418, step = 5401 (1.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.7868\n",
      "INFO:tensorflow:loss = 0.6525755, step = 5501 (1.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.2615\n",
      "INFO:tensorflow:loss = 0.411766, step = 5601 (1.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.9544\n",
      "INFO:tensorflow:loss = 0.54544115, step = 5701 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0052\n",
      "INFO:tensorflow:loss = 0.5313549, step = 5801 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.8049\n",
      "INFO:tensorflow:loss = 0.46123892, step = 5901 (1.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.6862\n",
      "INFO:tensorflow:loss = 0.4946898, step = 6001 (1.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.7181\n",
      "INFO:tensorflow:loss = 0.5920841, step = 6101 (1.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.652\n",
      "INFO:tensorflow:loss = 0.478177, step = 6201 (1.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.9094\n",
      "INFO:tensorflow:loss = 0.6013, step = 6301 (1.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.1995\n",
      "INFO:tensorflow:loss = 0.36684826, step = 6401 (1.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4152\n",
      "INFO:tensorflow:loss = 0.56415313, step = 6501 (1.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5054\n",
      "INFO:tensorflow:loss = 0.45677805, step = 6601 (1.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.3334\n",
      "INFO:tensorflow:loss = 0.28749758, step = 6701 (1.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2213\n",
      "INFO:tensorflow:loss = 0.46626914, step = 6801 (1.231 sec)\n"
     ]
    }
   ],
   "source": [
    "bow_sd_comment, bow_nd_comment, bow_comment_preds, ct = train_and_test_bow(comment_set)\n",
    "bow_sd_wiki, bow_nd_wiki, bow_wiki_preds, wt          = train_and_test_bow(wiki_set)\n",
    "bow_sd_merge, bow_nd_merge, bow_merge_preds, mt       = train_and_test_bow(merge_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bow_sd_comment\", bow_sd_comment)\n",
    "print(\"\\nbow_nd_comment\", bow_nd_comment)\n",
    "print(ct)\n",
    "\n",
    "print(\"\\n\\nbow_sd_wiki\", bow_sd_wiki)\n",
    "print(\"\\nbow_nd_wiki\", bow_nd_wiki)\n",
    "print(wt)\n",
    "\n",
    "print(\"\\n\\nbow_sd_merge\",bow_sd_merge)\n",
    "print(\"\\nbow_nd_merge\", bow_nd_merge)\n",
    "print(mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBED_DIM = 90\n",
    "CELL_SIZE = 40\n",
    "RNN_NUM = 3\n",
    "learning_rate = 0\n",
    "dropout_keep_prob = 0\n",
    "lam = 1\n",
    "train_embedding = False\n",
    "embedding_path = \"\"\n",
    "model_dir = \"runs\"\n",
    "summaries_dir = \"summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 10)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_test_cnn(ts):\n",
    "    start_time = time.time()\n",
    "    x, _, y, y_ext = ts.trains\n",
    "    dev_x, _, dev_y, dev_y_ext = ts.devs\n",
    "    test_set = TrainingSet()\n",
    "\n",
    "    test_set.load_test_arr(test_arr,tokenizer)\n",
    "    test_set.set_vocab(ts.vocab)\n",
    "    test_set.prep_sents(PAD_LEN)\n",
    "    test_set.prep_sets()\n",
    "    \n",
    "    test_x, test_ns, test_y, test_y_ext = test_set.ext_tests\n",
    "    sd_test_x, _, sd_test_y, sd_test_y_ext = ts.sds\n",
    "    \n",
    "    # Training\n",
    "    # ==================================================\n",
    "    last_path = \"\"\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "          log_device_placement=FLAGS.log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x.shape[1],\n",
    "                num_classes=len(labels),\n",
    "                vocab=ts.vocab,\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters=FLAGS.num_filters,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Dev summaries\n",
    "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "            # Write vocabulary\n",
    "            ts.vocab.write_flat_file(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            def train_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            def dev_step(x_batch, y_batch, writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "\n",
    "            # Generate batches\n",
    "            batches = batch_iter(\n",
    "                list(zip(x, y_ext)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(dev_x, dev_y_ext, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    last_path = path\n",
    "    def test(test_x, test_y):\n",
    "    #     checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "        checkpoint_file = path\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            session_conf = tf.ConfigProto(\n",
    "              allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "              log_device_placement=FLAGS.log_device_placement)\n",
    "            sess = tf.Session(config=session_conf)\n",
    "            with sess.as_default():\n",
    "                # Load the saved meta graph and restore variables\n",
    "                saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "                saver.restore(sess, checkpoint_file)\n",
    "\n",
    "                # Get the placeholders from the graph by name\n",
    "                input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "                # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "                dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "                # Tensors we want to evaluate\n",
    "                predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "                # Generate batches for one epoch\n",
    "                batches = batch_iter(list(test_x), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "                # Collect the predictions here\n",
    "                all_predictions = []\n",
    "\n",
    "                for x_test_batch in batches:\n",
    "                    batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "                    all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "        # Print accuracy if y_test is defined\n",
    "        if test_y is not None:\n",
    "            correct_predictions = float(sum(all_predictions == test_y))\n",
    "            print(\"Total number of test examples: {}\".format(len(test_y)))\n",
    "            print(\"Accuracy: {:g}\".format(correct_predictions/float(len(test_y))))\n",
    "            return correct_predictions/float(len(test_y))\n",
    "    same_domain = test(sd_test_x, sd_test_y)\n",
    "    new_domain = test(test_x, test_y)\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    return same_domain, new_domain, all_predictions, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_sd_comment, cnn_nd_comment, cnn_preds_comment, ct = train_and_test_cnn(comment_set)\n",
    "cnn_sd_wiki, cnn_nd_wiki, cnn_preds_wiki, wt          = train_and_test_cnn(wiki_set)\n",
    "cnn_sd_merge, cnn_nd_merge, cnn_preds_merge, mt       = train_and_test_cnn(merge_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sd comment Accuracy: {:g}\".format(same_domain_comment))\n",
    "print(\"nd comment Accuracy: {:g}\".format(new_domain_comment))\n",
    "print(ct)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"sd wiki Accuracy: {:g}\".format(same_domain_wiki))\n",
    "print(\"nd wiki Accuracy: {:g}\".format(new_domain_wiki))\n",
    "print(wt)\n",
    "print(\"\\n\")\n",
    "print(\"sd merge Accuracy: {:g}\".format(same_domain_merge))\n",
    "print(\"nd merge Accuracy: {:g}\".format(new_domain_merge))\n",
    "print(mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- https://www.tensorflow.org/tutorials/text_classification_with_tf_hub\n",
    "-- https://github.com/dennybritz/cnn-text-classification-tf\n",
    "-- With word2vec\n",
    "-- https://github.com/cahya-wirawan/cnn-text-classification-tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
